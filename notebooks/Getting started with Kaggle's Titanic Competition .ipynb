{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting started: Kaggle's Titanic Competition\n",
    "\n",
    "Kaggle is already established as the best place which hosts machine learning competitions. If you do not know it already, then it's time to do it.\n",
    "\n",
    "__[Titanic Competition](https://www.kaggle.com/c/titanic)__ is perhaps the first competition which one should try. Of course, if you are already an experienced data scientist, than you can skip to an advanced competition.\n",
    "\n",
    "The purpose of the competition is to learn if a passenger has survived or not. We illustrate some steps and ideas one can apply to compete in this learning competition using the available tools one can find in rapaio library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%jars /home/ati/work/out/artifacts/rapaio_jar/rapaio.jar /home/ati/work/out/artifacts/rapaio_jar/fastutil-8.2.3.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.util.stream.*;\n",
    "import java.util.function.*;\n",
    "\n",
    "import rapaio.datasets.*;\n",
    "import rapaio.data.*;\n",
    "import rapaio.sys.*;\n",
    "import rapaio.graphics.*;\n",
    "import rapaio.graphics.plot.*;\n",
    "import static rapaio.graphics.Plotter.*;\n",
    "import rapaio.io.*;\n",
    "import rapaio.core.*;\n",
    "import rapaio.core.distributions.*;\n",
    "import rapaio.core.tests.*;\n",
    "import rapaio.core.tools.*;\n",
    "import rapaio.experiment.core.tools.*;\n",
    "import rapaio.core.stat.*;\n",
    "import rapaio.ml.eval.*;\n",
    "import rapaio.ml.common.*;\n",
    "import rapaio.ml.classifier.*;\n",
    "import rapaio.experiment.ml.classifier.tree.*;\n",
    "import rapaio.experiment.ml.classifier.ensemble.*;\n",
    "\n",
    "import rapaio.data.filter.*;\n",
    "import rapaio.data.filter.var.*;\n",
    "import rapaio.data.filter.frame.*;\n",
    "\n",
    "import rapaio.experiment.ml.eval.*;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rapaio.printer.standard.StandardPrinter@4ad1349f"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WS.getPrinter().withGraphicShape(800, 600);\n",
    "WS.getPrinter().withTextWidth(100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Get the data\n",
    "\n",
    "The purpose of the competition is to predict which passengers have survived or not. The available data has two parts. The first part consists in a data set which contains what happened with some passengers and some related information like sex, cabin, age, class, etc. This data set contains information regarding their survival. The purpose why this data set contains survival data is because it will be used to train a model which learns how to decide if a passenger survives or not. This is the `train.csv`. The other file is a data set which contains data about another set of passenger, this time without knowing if they survived or not. They contain, however an identification number. This data set is `test.csv` and this is used to make predictions. Those predictions should be similar with the provided `gendermodel.csv`.\n",
    "\n",
    "We also have to take a look of the data description provided on __[contest dedicated page](https://www.kaggle.com/c/titanic/data)__:\n",
    "\n",
    "```\n",
    "VARIABLE DESCRIPTIONS:\n",
    "survival Survival\n",
    "(0 = No; 1 = Yes)\n",
    "pclass Passenger Class\n",
    "(1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    "name Name\n",
    "sex Sex\n",
    "age Age\n",
    "sibsp Number of Siblings/Spouses Aboard\n",
    "parch Number of Parents/Children Aboard\n",
    "ticket Ticket Number\n",
    "fare Passenger Fare\n",
    "cabin Cabin\n",
    "embarked Port of Embarkation\n",
    "(C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "\n",
    "SPECIAL NOTES:\n",
    "Pclass is a proxy for socio-economic status (SES)\n",
    "1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower\n",
    "\n",
    "Age is in Years; Fractional if Age less than One (1)\n",
    "If the Age is Estimated, it is in the form xx.5\n",
    "\n",
    "With respect to the family relation variables (i.e. sibsp and parch)\n",
    "some relations were ignored. The following are the definitions used\n",
    "for sibsp and parch.\n",
    "\n",
    "Sibling: Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic\n",
    "Spouse: Husband or Wife of Passenger Aboard Titanic (Mistresses and Fiances Ignored)\n",
    "Parent: Mother or Father of Passenger Aboard Titanic\n",
    "Child: Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic\n",
    "\n",
    "Other family relatives excluded from this study include cousins,\n",
    "nephews/nieces, aunts/uncles, and in-laws. Some children travelled\n",
    "only with a nanny, therefore parch=0 for them. As well, some\n",
    "travelled with very close friends or neighbors in a village, however,\n",
    "the definitions do not support such relations.\n",
    "```\n",
    "\n",
    "The first step in our adventure is to download those 3 data file in *csv* format. You can do it from __[data section](https://www.kaggle.com/c/titanic/data)__ of the competition. Let's suppose you downloaded somewhere in a local folder. We will name this folder `data` folder, and actually it can have any name you would like.\n",
    "\n",
    "### 1.2 Read train data from csv file\n",
    "\n",
    "Because the data is small we can load the whole data in memory with no problems.\n",
    "\n",
    "Let's see how we can load the data into memory. In rapaio the sets of data are loaded into the form of *frames* (`rapaio.data.Frame`). A frame is basically a tabular data, with columns for each variable (feature) and rows for each instance (in our case for each passenger).\n",
    "\n",
    "\n",
    "A first try of loading the train data set and see what has happened is the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Summary\n",
      "=============\n",
      "* rowCount: 891\n",
      "* complete: 183/891\n",
      "* varCount: 12\n",
      "* varNames: \n",
      "\n",
      " 0. PassengerId : double  |  4.         Sex : nominal |  8.      Ticket : nominal | \n",
      " 1.    Survived : binary  |  5.         Age : double  |  9.        Fare : double  | \n",
      " 2.      Pclass : double  |  6.       SibSp : double  | 10.       Cabin : nominal | \n",
      " 3.        Name : nominal |  7.       Parch : double  | 11.    Embarked : nominal | \n",
      "\n",
      "      PassengerId         Survived           Pclass \n",
      "   Min. :   1.000     Min. : 0.000     Min. : 1.000 \n",
      "1st Qu. : 223.500  1st Qu. : 0.000  1st Qu. : 2.000 \n",
      " Median : 446.000   Median : 0.000   Median : 3.000 \n",
      "   Mean : 446.000     Mean : 0.384     Mean : 2.309 \n",
      "2nd Qu. : 668.500  2nd Qu. : 1.000  2nd Qu. : 3.000 \n",
      "   Max. : 891.000     Max. : 1.000     Max. : 3.000 \n",
      "                                                    \n",
      "                                                       Name           Sex               Age \n",
      "                            \"Braund, Mr. Owen Harris\" :   1    male : 577     Min. :  0.420 \n",
      "\"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\" :   1  female : 314  1st Qu. : 20.125 \n",
      "                             \"Heikkinen, Miss. Laina\" :   1                 Median : 28.000 \n",
      "       \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\" :   1                   Mean : 29.699 \n",
      "                           \"Allen, Mr. William Henry\" :   1                2nd Qu. : 38.000 \n",
      "                                   \"Moran, Mr. James\" :   1                   Max. : 80.000 \n",
      "                                              (Other) : 885                   NA's :    177 \n",
      "          SibSp            Parch          Ticket               Fare              Cabin    Embarked \n",
      "   Min. : 0.000     Min. : 0.000    347082 :   7     Min. :   0.000           G6 :   4     S : 644 \n",
      "1st Qu. : 0.000  1st Qu. : 0.000      1601 :   7  1st Qu. :   7.910  C23 C25 C27 :   4     C : 168 \n",
      " Median : 0.000   Median : 0.000  CA. 2343 :   7   Median :  14.454      B96 B98 :   4     Q :  77 \n",
      "   Mean : 0.523     Mean : 0.382   3101295 :   6     Mean :  32.204          F33 :   3  NA's :   2 \n",
      "2nd Qu. : 1.000  2nd Qu. : 0.000   CA 2144 :   6  2nd Qu. :  31.000         E101 :   3             \n",
      "   Max. : 8.000     Max. : 6.000    347088 :   6     Max. : 512.329      (Other) : 183             \n",
      "                                   (Other) : 852                            NA's : 687             \n",
      "\n"
     ]
    }
   ],
   "source": [
    "String root = \"/home/ati/work/rapaio-kaggle/src/titanic/\";\n",
    "Csv.instance().read(root + \"train.csv\").printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we interpret the output of the frame's summary?\n",
    "\n",
    "* We loaded a frame which has $891$ rows and $12$ columns (variables)\n",
    "* From all the rows, $889$ are complete (non missing data)\n",
    "* The name of the variables are listed, together with their types\n",
    "* It follows a data summary for the frame: 6 number summary for numeric variables, most frequent levels for nominal variables\n",
    "\n",
    "Let's inspect each variable and see how it fits our needs.\n",
    "\n",
    "**PassengedId**\n",
    "\n",
    "The type for this variable is index (integer values). This field looks like an identifier for the passenger, so from our point of view the sorting is not required. What we can do, but is not required, is to change the field type to nominal. Anyway, we do not need this field for learning since it should be unique for each instance, thus the predictive power is null. We will ignore it for now since we will not consider it for learning\n",
    "\n",
    "**Survived**\n",
    "\n",
    "This is our target variable. It is parsed as binary, but since we do classification, we will change it's type to nominal. We do that directly from the csv parsing, by indicating that we want Survived parsed as nominal variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Summary\n",
      "=============\n",
      "* rowCount: 891\n",
      "* complete: 183/891\n",
      "* varCount: 12\n",
      "* varNames: \n",
      "\n",
      " 0. PassengerId : double  |  4.         Sex : nominal |  8.      Ticket : nominal | \n",
      " 1.    Survived : nominal |  5.         Age : double  |  9.        Fare : double  | \n",
      " 2.      Pclass : double  |  6.       SibSp : double  | 10.       Cabin : nominal | \n",
      " 3.        Name : nominal |  7.       Parch : double  | 11.    Embarked : nominal | \n",
      "\n",
      "      PassengerId  Survived           Pclass                                                         Name \n",
      "   Min. :   1.000   0 : 549     Min. : 1.000                              \"Braund, Mr. Owen Harris\" :   1 \n",
      "1st Qu. : 223.500   1 : 342  1st Qu. : 2.000  \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\" :   1 \n",
      " Median : 446.000             Median : 3.000                               \"Heikkinen, Miss. Laina\" :   1 \n",
      "   Mean : 446.000               Mean : 2.309         \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\" :   1 \n",
      "2nd Qu. : 668.500            2nd Qu. : 3.000                             \"Allen, Mr. William Henry\" :   1 \n",
      "   Max. : 891.000               Max. : 3.000                                     \"Moran, Mr. James\" :   1 \n",
      "                                                                                            (Other) : 885 \n",
      "         Sex               Age            SibSp            Parch          Ticket               Fare \n",
      "  male : 577     Min. :  0.420     Min. : 0.000     Min. : 0.000    347082 :   7     Min. :   0.000 \n",
      "female : 314  1st Qu. : 20.125  1st Qu. : 0.000  1st Qu. : 0.000      1601 :   7  1st Qu. :   7.910 \n",
      "               Median : 28.000   Median : 0.000   Median : 0.000  CA. 2343 :   7   Median :  14.454 \n",
      "                 Mean : 29.699     Mean : 0.523     Mean : 0.382   3101295 :   6     Mean :  32.204 \n",
      "              2nd Qu. : 38.000  2nd Qu. : 1.000  2nd Qu. : 0.000   CA 2144 :   6  2nd Qu. :  31.000 \n",
      "                 Max. : 80.000     Max. : 8.000     Max. : 6.000    347088 :   6     Max. : 512.329 \n",
      "                 NA's :    177                                     (Other) : 852                    \n",
      "            Cabin    Embarked \n",
      "         G6 :   4     S : 644 \n",
      "C23 C25 C27 :   4     C : 168 \n",
      "    B96 B98 :   4     Q :  77 \n",
      "        F33 :   3  NA's :   2 \n",
      "       E101 :   3             \n",
      "    (Other) : 183             \n",
      "       NA's : 687             \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Csv.instance()\n",
    ".withTypes(VType.NOMINAL, \"Survived\")\n",
    ".read(root + \"train.csv\")\n",
    ".printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And notice how type of the `Survived` variable changed to nominal.\n",
    "\n",
    "\n",
    "**Pclass**\n",
    "\n",
    "This variable has index type. We can keep it like it is or we can change it to nominal. Both ways can be useful. For example parsed as index could give an interpretation to the order. We can say that somehow, because of ordering class 1 is lower than class 2, and class 2 is between classes 1 and 3. At the same time we can keep it as nominal if we do not want to use the ordering. Let's choose nominal for now, considering that 1,2 and 3 are just labels for type of tickets, with no other meaning attached. We proceed in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Summary\n",
      "=============\n",
      "* rowCount: 891\n",
      "* complete: 183/891\n",
      "* varCount: 12\n",
      "* varNames: \n",
      "\n",
      " 0. PassengerId : double  |  4.         Sex : nominal |  8.      Ticket : nominal | \n",
      " 1.    Survived : nominal |  5.         Age : double  |  9.        Fare : double  | \n",
      " 2.      Pclass : nominal |  6.       SibSp : double  | 10.       Cabin : nominal | \n",
      " 3.        Name : nominal |  7.       Parch : double  | 11.    Embarked : nominal | \n",
      "\n",
      "      PassengerId  Survived   Pclass                                                         Name \n",
      "   Min. :   1.000   0 : 549  3 : 491                              \"Braund, Mr. Owen Harris\" :   1 \n",
      "1st Qu. : 223.500   1 : 342  1 : 216  \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\" :   1 \n",
      " Median : 446.000            2 : 184                               \"Heikkinen, Miss. Laina\" :   1 \n",
      "   Mean : 446.000                            \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\" :   1 \n",
      "2nd Qu. : 668.500                                                \"Allen, Mr. William Henry\" :   1 \n",
      "   Max. : 891.000                                                        \"Moran, Mr. James\" :   1 \n",
      "                                                                                    (Other) : 885 \n",
      "         Sex               Age            SibSp            Parch          Ticket               Fare \n",
      "  male : 577     Min. :  0.420     Min. : 0.000     Min. : 0.000    347082 :   7     Min. :   0.000 \n",
      "female : 314  1st Qu. : 20.125  1st Qu. : 0.000  1st Qu. : 0.000      1601 :   7  1st Qu. :   7.910 \n",
      "               Median : 28.000   Median : 0.000   Median : 0.000  CA. 2343 :   7   Median :  14.454 \n",
      "                 Mean : 29.699     Mean : 0.523     Mean : 0.382   3101295 :   6     Mean :  32.204 \n",
      "              2nd Qu. : 38.000  2nd Qu. : 1.000  2nd Qu. : 0.000   CA 2144 :   6  2nd Qu. :  31.000 \n",
      "                 Max. : 80.000     Max. : 8.000     Max. : 6.000    347088 :   6     Max. : 512.329 \n",
      "                 NA's :    177                                     (Other) : 852                    \n",
      "            Cabin    Embarked \n",
      "         G6 :   4     S : 644 \n",
      "C23 C25 C27 :   4     C : 168 \n",
      "    B96 B98 :   4     Q :  77 \n",
      "        F33 :   3  NA's :   2 \n",
      "       E101 :   3             \n",
      "    (Other) : 183             \n",
      "       NA's : 687             \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Csv.instance()\n",
    ".withTypes(VType.NOMINAL, \"Survived\", \"Pclass\")\n",
    ".read(root + \"train.csv\")\n",
    ".printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we append the variable name after `Survived`. This is possible since the `withTypes` method specify a type, and follows a dynamic array of strings, for the names of variables.\n",
    "\n",
    "**Name**\n",
    "\n",
    "This is the passenger names and the values are unique. As it is, the predictive power of this field is null. We keep it as it is. Note that it contains valuable information, but not in this direct form.\n",
    "\n",
    "**Sex**\n",
    "\n",
    "This field specifies the gender of the passenger. We have $577$ males and $314$ females.\n",
    "\n",
    "**Age**\n",
    "\n",
    "This field specifies the age of an passenger. We would expect that to parse this variable as numeric or at leas index, but is nominal. Why that happened? Notice that the values looks like numbers. But the first value (the most frequent one, $117$ instances) has nothing specified. Well, the variable is nominal has to do with how *Csv* parsing handles missing values. By default, the *csv* parsing considers as missing values only the string \"?\". But the most frequent value in this field is empty string \"\". This means that empty string is not considered a missing value. Because empty string can't produce numbers from parsing, the variable is *promoted* to nominal.\n",
    "\n",
    "We can customize the missing value handling by specifying the valid strings for that purpose. We use `.useNAValues(String...naValues)` to tell the parser all the valid strings which are missing values. In our case we want just the empty string to be a missing value. When the parser will found an empty string it will set the variable value as missing value. It will *not promote* variable to nominal, since a missing value is a legal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Summary\n",
      "=============\n",
      "* rowCount: 891\n",
      "* complete: 183/891\n",
      "* varCount: 12\n",
      "* varNames: \n",
      "\n",
      " 0. PassengerId : double  |  4.         Sex : nominal |  8.      Ticket : nominal | \n",
      " 1.    Survived : nominal |  5.         Age : double  |  9.        Fare : double  | \n",
      " 2.      Pclass : nominal |  6.       SibSp : double  | 10.       Cabin : nominal | \n",
      " 3.        Name : nominal |  7.       Parch : double  | 11.    Embarked : nominal | \n",
      "\n",
      "      PassengerId  Survived   Pclass                                                         Name \n",
      "   Min. :   1.000   0 : 549  3 : 491                              \"Braund, Mr. Owen Harris\" :   1 \n",
      "1st Qu. : 223.500   1 : 342  1 : 216  \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\" :   1 \n",
      " Median : 446.000            2 : 184                               \"Heikkinen, Miss. Laina\" :   1 \n",
      "   Mean : 446.000                            \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\" :   1 \n",
      "2nd Qu. : 668.500                                                \"Allen, Mr. William Henry\" :   1 \n",
      "   Max. : 891.000                                                        \"Moran, Mr. James\" :   1 \n",
      "                                                                                    (Other) : 885 \n",
      "         Sex               Age            SibSp            Parch          Ticket               Fare \n",
      "  male : 577     Min. :  0.420     Min. : 0.000     Min. : 0.000    347082 :   7     Min. :   0.000 \n",
      "female : 314  1st Qu. : 20.125  1st Qu. : 0.000  1st Qu. : 0.000      1601 :   7  1st Qu. :   7.910 \n",
      "               Median : 28.000   Median : 0.000   Median : 0.000  CA. 2343 :   7   Median :  14.454 \n",
      "                 Mean : 29.699     Mean : 0.523     Mean : 0.382   3101295 :   6     Mean :  32.204 \n",
      "              2nd Qu. : 38.000  2nd Qu. : 1.000  2nd Qu. : 0.000   CA 2144 :   6  2nd Qu. :  31.000 \n",
      "                 Max. : 80.000     Max. : 8.000     Max. : 6.000    347088 :   6     Max. : 512.329 \n",
      "                 NA's :    177                                     (Other) : 852                    \n",
      "            Cabin    Embarked \n",
      "         G6 :   4     S : 644 \n",
      "C23 C25 C27 :   4     C : 168 \n",
      "    B96 B98 :   4     Q :  77 \n",
      "        F33 :   3  NA's :   2 \n",
      "       E101 :   3             \n",
      "    (Other) : 183             \n",
      "       NA's : 687             \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Csv.instance()\n",
    ".withNAValues(\"\")\n",
    ".withTypes(VType.NOMINAL, \"Survived\", \"Pclass\")\n",
    ".read(root + \"train.csv\")\n",
    ".printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice what happened: *Age* field is now numeric and it contains $177$ missing values.\n",
    "\n",
    "**SibSp**\n",
    "\n",
    "It's meaning is \"siblings/spouses\". It's parsed as index, which is natural. In pathological cases with sick imagination we can consider a \"quarter of a wife\" for example.\n",
    "\n",
    "**Parch**\n",
    "\n",
    "It's meaning is \"parents/children\". It is naturally parsed as index.\n",
    "\n",
    "**Ticket**\n",
    "\n",
    "This is the code of the ticket. Probably a family can have the same ticket, thus must be the reason why the frequencies have values up to $$7$$. This field is nominal. It has low predictive power used directly. Perhaps contains valuable information, but used directly in row format would not help much.\n",
    "\n",
    "**Fare**\n",
    "\n",
    "This is the price for passenger fare and should be numeric, like it is.\n",
    "\n",
    "**Cabin**\n",
    "\n",
    "Code of the passenger's cabin, parsed as nominal. Same notes as for `Ticket` variable.\n",
    "\n",
    "**Embarked**\n",
    "\n",
    "Code for the embarking city, which could be: C = Cherbourg, Q = Queenstown, S = Southampton. It's parsed as nominal and has $2$ missing values.\n",
    "\n",
    "If we are content with our parsing, we load data into a data frame for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Summary\n",
      "=============\n",
      "* rowCount: 891\n",
      "* complete: 183/891\n",
      "* varCount: 12\n",
      "* varNames: \n",
      "\n",
      " 0. PassengerId : double  |  4.         Sex : nominal |  8.      Ticket : nominal | \n",
      " 1.    Survived : nominal |  5.         Age : double  |  9.        Fare : double  | \n",
      " 2.      Pclass : nominal |  6.       SibSp : double  | 10.       Cabin : nominal | \n",
      " 3.        Name : nominal |  7.       Parch : double  | 11.    Embarked : nominal | \n",
      "\n",
      "      PassengerId  Survived   Pclass                                                         Name \n",
      "   Min. :   1.000   0 : 549  3 : 491                              \"Braund, Mr. Owen Harris\" :   1 \n",
      "1st Qu. : 223.500   1 : 342  1 : 216  \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\" :   1 \n",
      " Median : 446.000            2 : 184                               \"Heikkinen, Miss. Laina\" :   1 \n",
      "   Mean : 446.000                            \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\" :   1 \n",
      "2nd Qu. : 668.500                                                \"Allen, Mr. William Henry\" :   1 \n",
      "   Max. : 891.000                                                        \"Moran, Mr. James\" :   1 \n",
      "                                                                                    (Other) : 885 \n",
      "         Sex               Age            SibSp            Parch          Ticket               Fare \n",
      "  male : 577     Min. :  0.420     Min. : 0.000     Min. : 0.000    347082 :   7     Min. :   0.000 \n",
      "female : 314  1st Qu. : 20.125  1st Qu. : 0.000  1st Qu. : 0.000      1601 :   7  1st Qu. :   7.910 \n",
      "               Median : 28.000   Median : 0.000   Median : 0.000  CA. 2343 :   7   Median :  14.454 \n",
      "                 Mean : 29.699     Mean : 0.523     Mean : 0.382   3101295 :   6     Mean :  32.204 \n",
      "              2nd Qu. : 38.000  2nd Qu. : 1.000  2nd Qu. : 0.000   CA 2144 :   6  2nd Qu. :  31.000 \n",
      "                 Max. : 80.000     Max. : 8.000     Max. : 6.000    347088 :   6     Max. : 512.329 \n",
      "                 NA's :    177                                     (Other) : 852                    \n",
      "            Cabin    Embarked \n",
      "         G6 :   4     S : 644 \n",
      "C23 C25 C27 :   4     C : 168 \n",
      "    B96 B98 :   4     Q :  77 \n",
      "        F33 :   3  NA's :   2 \n",
      "       E101 :   3             \n",
      "    (Other) : 183             \n",
      "       NA's : 687             \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Frame train = Csv.instance().withNAValues(\"\").withTypes(VType.NOMINAL, \"Survived\", \"Pclass\")\n",
    ".read(root + \"train.csv\");\n",
    "train.printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Read test data from *csv* file\n",
    "\n",
    "Once we have a training frame we can load also the test data. We do that to take a look at the frame and because data is small and there is no memory or time problem cost associated with it. To avoid adding again the *csv* options and to get identical levels nominal variables, we use a different way to parse the data set. We specify variable types by frame templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Frame test = Csv.instance().withNAValues(\"\").withTemplate(train).read(root + \"test.csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead to specify again the preferred types for variables, we use train frame as a template for variable types. This has also the side effect that the encoding of categorical variables is identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Summary\n",
      "=============\n",
      "* rowCount: 418\n",
      "* complete: 87/418\n",
      "* varCount: 11\n",
      "* varNames: \n",
      "\n",
      " 0. PassengerId : double  |  4.         Age : double  |  8.        Fare : double  | \n",
      " 1.      Pclass : nominal |  5.       SibSp : double  |  9.       Cabin : nominal | \n",
      " 2.        Name : nominal |  6.       Parch : double  | 10.    Embarked : nominal | \n",
      " 3.         Sex : nominal |  7.      Ticket : nominal | \n",
      "\n",
      "       PassengerId   Pclass                                                  Name           Sex \n",
      "   Min. :  892.000  3 : 218                          \"Connolly, Miss. Kate\" :   1    male : 266 \n",
      "1st Qu. :  996.250  1 : 107                              \"Kelly, Mr. James\" :   1  female : 152 \n",
      " Median : 1100.500  2 :  93              \"Wilkes, Mrs. James (Ellen Needs)\" :   1               \n",
      "   Mean : 1100.500                              \"Myles, Mr. Thomas Francis\" :   1               \n",
      "2nd Qu. : 1204.750                                       \"Wirz, Mr. Albert\" :   1               \n",
      "   Max. : 1309.000           \"Hirvonen, Mrs. Alexander (Helga E Lindqvist)\" :   1               \n",
      "                                                                    (Other) : 412               \n",
      "             Age            SibSp            Parch          Ticket               Fare                  Cabin \n",
      "   Min. :  0.170     Min. : 0.000     Min. : 0.000  PC 17608 :   5     Min. :   0.000  B57 B59 B63 B66 :   3 \n",
      "1st Qu. : 21.000  1st Qu. : 0.000  1st Qu. : 0.000  CA. 2343 :   4  1st Qu. :   7.896      C23 C25 C27 :   2 \n",
      " Median : 27.000   Median : 0.000   Median : 0.000    113503 :   4   Median :  14.454               F4 :   2 \n",
      "   Mean : 30.273     Mean : 0.447     Mean : 0.392    347077 :   3     Mean :  35.627              C78 :   2 \n",
      "2nd Qu. : 39.000  2nd Qu. : 1.000  2nd Qu. : 0.000     16966 :   3  2nd Qu. :  31.500              E34 :   2 \n",
      "   Max. : 76.000     Max. : 8.000     Max. : 9.000  PC 17483 :   3     Max. : 512.329          (Other) :  78 \n",
      "   NA's :     86                                     (Other) : 396     NA's :       1             NA's : 327 \n",
      "Embarked \n",
      " S : 270 \n",
      " C : 102 \n",
      " Q :  46 \n",
      "         \n",
      "         \n",
      "         \n",
      "         \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note that we don't have *Survived* variable anymore. This is correct since this is what we have to predict. Note also that the types for the remaining variables are the same with training data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build simple models\n",
    "\n",
    "### 2.1 Build a majority model\n",
    "\n",
    "To make a first submission we will build a very simple model, which classifies with a single value all instances. This value is the majority label.\n",
    "\n",
    "Let's inspect at how target variable look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1 \n",
      "   -   - \n",
      " 549 342 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "DVector.fromCounts(false, train.rvar(\"Survived\")).printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already new from the summary, the number of passengers who didn't survived is lower than those who did. Let's see percentages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1 \n",
      "         -         - \n",
      " 0.6161616 0.3838384 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "DVector.fromCounts(false, train.rvar(\"Survived\")).normalize().printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that there are about $61\\%$ of passengers who did not survived. We will create a submit data set, which we will save for later submission. How we can do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VarNominal prediction = VarNominal.from(test.rowCount(), row -> \"0\").withName(\"Survived\");\n",
    "Frame submit = SolidFrame.byVars(test.rvar(\"PassengerId\"), prediction);\n",
    "\n",
    "Csv.instance().withQuotes(false).write(submit, root + \"majority_submit.csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first line we created a new nominal variable. The size of the new variable is the number of rows from the test frame. For each row we produce the same label `\"0\"`. We name this variable `Survived`.\n",
    "\n",
    "In the second line we created a new frame taking the variable named `PassengerId` from the test data set and the new prediction variable.\n",
    "\n",
    "In the last line we wrote a new csv file with the csv parsing utility, taking care to not write quotes. We can submit this file and see which are the results.\n",
    "\n",
    "![Submission result with majority classifier](images/titanic-majority-submit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Build a simple gender model\n",
    "\n",
    "It has been said that \"women and children first\" really happened during Titanic tragedy. If this was true or not, we do not know. But we can use data to see if we are hearing the same story. For now we will take the gender and see if it had an influence. We will build a contingency table for variables `Sex` and `Survived`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0   1 total \n",
      "  male 468 109  577  \n",
      "female  81 233  314  \n",
      " total 549 342  891  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "DTable.fromCounts(train.rvar(\"Sex\"), train.rvar(\"Survived\"), false).printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On rows we have levels of `Sex` variable. On columns we have levels of `Sex` variable. Cells are computed as counts. What we see is that there are a lot of men who did not survived and a lot of women who does. We will normalize on rows to take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0         1 total \n",
      "  male 0.8110919 0.1889081   1   \n",
      "female 0.2579618 0.7420382   1   \n",
      " total 1.0690536 0.9309464   2   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "DTable.fromCounts(train.rvar(\"Sex\"), train.rvar(\"Survived\"), false).normalizeOnRows().printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that men survived with a rate of $0.19$ and women with $0.74$. The values are so obvious, we need no hypothesis testing to check that this variable is significant for classification. We will build a simple model where we predict as survived all the women and not survived all the men."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Var prediction = VarNominal.from(test.rowCount(), row -> test.getLabel(row, \"Sex\").equals(\"male\") \n",
    "? \"0\" : \"1\").withName(\"Survived\");\n",
    "Frame submit = SolidFrame.byVars(test.rvar(\"PassengerId\"), prediction);\n",
    "Csv.instance().withQuotes(false).write(submit, root + \"gender_submit.csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__![Submission result with gender classifier](images/titanic-gender-submit.png)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tree model\n",
    "\n",
    "Building models in the manual way is often not the way to go. This process is tedious and time consuming. There are already built automated procedures, which incorporate miscellaneous approaches to learn a classifier. One of the often used models is the decision tree. Decision trees are greedy local approximations build in a recursive greedy fashion. Often the split decision at node level uses a single feature. At leave nodes a simple majority classifier creates the classification rule.\n",
    "\n",
    "### 3.1 Gender model with decision tree\n",
    "Initially we will build a CART decision tree using as input feature the *Sex* variable. We do this to exemplify how a manual rule can be created in an automated fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTree model\n",
      "================\n",
      "\n",
      "Description:\n",
      "CTree {varSelector=VarSelector[ALL];\n",
      "minCount=1;\n",
      "maxDepth=-1;\n",
      "tests=INT:NumericBinary,NOMINAL:NominalBinary,BINARY:BinaryBinary,DOUBLE:NumericBinary;\n",
      "func=GiniGain;\n",
      "split=ToAllWeighted;\n",
      "}\n",
      "\n",
      "Capabilities:\n",
      "types inputs/targets: BINARY,INT,NOMINAL,DOUBLE/NOMINAL\n",
      "counts inputs/targets: [1,1000000] / [1,1]\n",
      "missing inputs/targets: true/false\n",
      "\n",
      "Learned model:\n",
      "input vars: \n",
      "0. Sex : NOMINAL  | \n",
      "\n",
      "target vars:\n",
      "> Survived : NOMINAL [?,0,1]\n",
      "\n",
      "\n",
      "total number of nodes: 3\n",
      "total number of leaves: 2\n",
      "description:\n",
      "split, n/err, classes (densities) [* if is leaf / purity if not]\n",
      "\n",
      "|- 0. root    891/342 0 (0.616 0.384 ) [0.139648]\n",
      "|   |- 1. Sex = 'female'    314/81 1 (0.258 0.742 ) *\n",
      "|   |- 2. Sex != 'female'    577/109 0 (0.811 0.189 ) *\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Frame tr = train.mapVars(\"Survived,Sex\");\n",
    "CTree tree = CTree.newCART();\n",
    "tree.fit(tr, \"Survived\");\n",
    "tree.printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a closer look at the last three rows from the output, one can identify our manual rule. Basically the interpretation is: *\"all the females survived, all the males did not\"*. For exemplification purposes we build also the submit file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "// fit the tree to test data frame\n",
    "ClassResult pred = tree.predict(test);\n",
    "// build teh submission\n",
    "Frame submit = SolidFrame.byVars(test.rvar(\"PassengerId\"),pred.firstClasses().withName(\"Survived\"));\n",
    "// write to a submit file\n",
    "Csv.instance().withQuotes(false).write(submit, root + \"tree1-model.csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Enrich tree by using other features\n",
    "\n",
    "Our training data set has more than a single input feature. Thus We can state we didn't use all the information available. We will add now the class and embarking port and see how it behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTree model\n",
      "================\n",
      "\n",
      "Description:\n",
      "CTree {varSelector=VarSelector[ALL];\n",
      "minCount=1;\n",
      "maxDepth=-1;\n",
      "tests=INT:NumericBinary,NOMINAL:NominalBinary,BINARY:BinaryBinary,DOUBLE:NumericBinary;\n",
      "func=GiniGain;\n",
      "split=ToAllWeighted;\n",
      "}\n",
      "\n",
      "Capabilities:\n",
      "types inputs/targets: BINARY,INT,NOMINAL,DOUBLE/NOMINAL\n",
      "counts inputs/targets: [1,1000000] / [1,1]\n",
      "missing inputs/targets: true/false\n",
      "\n",
      "Learned model:\n",
      "input vars: \n",
      "0.      Sex : NOMINAL  | \n",
      "1.   Pclass : NOMINAL  | \n",
      "2. Embarked : NOMINAL  | \n",
      "\n",
      "target vars:\n",
      "> Survived : NOMINAL [?,0,1]\n",
      "\n",
      "\n",
      "total number of nodes: 17\n",
      "total number of leaves: 9\n",
      "description:\n",
      "split, n/err, classes (densities) [* if is leaf / purity if not]\n",
      "\n",
      "|- 0. root    891/342 0 (0.616 0.384 ) [0.139648]\n",
      "|   |- 1. Sex = 'female'    314/81 1 (0.258 0.742 ) [0.0204665]\n",
      "|   |   |- 3. Pclass = '2'    76/6 1 (0.079 0.921 ) [0.0003369]\n",
      "|   |   |   |- 7. Embarked = 'Q'    2/0 1 (0 1 ) *\n",
      "|   |   |   |- 8. Embarked != 'Q'    74/6 1 (0.081 0.919 ) [0.0013737]\n",
      "|   |   |   |   |- 13. Embarked = 'C'    7/0 1 (0 1 ) *\n",
      "|   |   |   |   |- 14. Embarked != 'C'    67/6 1 (0.09 0.91 ) *\n",
      "|   |   |- 4. Pclass != '2'    238/75 1 (0.315 0.685 ) [0.0009409]\n",
      "|   |   |   |- 9. Embarked = 'Q'    36/9 1 (0.262 0.738 ) *\n",
      "|   |   |   |- 10. Embarked != 'Q'    204/66 1 (0.324 0.676 ) [0.0348789]\n",
      "|   |   |   |   |- 15. Embarked = 'C'    68/9 1 (0.135 0.865 ) *\n",
      "|   |   |   |   |- 16. Embarked != 'C'    138/57 1 (0.416 0.584 ) *\n",
      "|   |- 2. Sex != 'female'    577/109 0 (0.811 0.189 ) [0.0020493]\n",
      "|   |   |- 5. Embarked = 'Q'    41/3 0 (0.927 0.073 ) *\n",
      "|   |   |- 6. Embarked != 'Q'    536/106 0 (0.802 0.198 ) [0.0049791]\n",
      "|   |   |   |- 11. Embarked = 'C'    95/29 0 (0.695 0.305 ) *\n",
      "|   |   |   |- 12. Embarked != 'C'    441/77 0 (0.825 0.175 ) *\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Frame tr = train.mapVars(\"Survived,Sex,Pclass,Embarked\");\n",
    "\n",
    "CTree tree = CTree.newCART();\n",
    "tree.fit(tr, \"Survived\");\n",
    "tree.printSummary();\n",
    "\n",
    "ClassResult pred = tree.predict(test);\n",
    "Frame submit = SolidFrame.byVars(test.rvar(\"PassengerId\"),pred.firstClasses().withName(\"Survived\"));\n",
    "Csv.instance().withQuotes(false).write(submit, root + \"tree2-model.csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree is much richer and there are more chances to be better. This is what happened after submission.\n",
    "\n",
    "__![Results after submission of enriched tree](images/titanic-tree2-submit.png)__\n",
    "\n",
    "**Nice!**. We advanced $704$ positions and improved our score with $0.01435$. On public leader board we have a nice $0.77990$ accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Overfitting with trees\n",
    "\n",
    "What about using other input features to improve our prediction accuracy? There are some of them which we can include directly, with no changes: *Age*,*Fare*,*SibSp* and *Parch*.\n",
    "\n",
    "We can change the script slightly, to include those new input features. But we can do better, we can use cross-validation to estimate what will happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CrossValidation with 10 folds\n",
      "CV  1:  acc=0.755556, mean=0.755556, se=NaN\n",
      "CV  2:  acc=0.853933, mean=0.804744, se=0.069563\n",
      "CV  3:  acc=0.786517, mean=0.798668, se=0.050302\n",
      "CV  4:  acc=0.775281, mean=0.792821, se=0.042703\n",
      "CV  5:  acc=0.797753, mean=0.793808, se=0.037048\n",
      "CV  6:  acc=0.775281, mean=0.790720, se=0.033989\n",
      "CV  7:  acc=0.741573, mean=0.783699, se=0.036163\n",
      "CV  8:  acc=0.775281, mean=0.782647, se=0.033612\n",
      "CV  9:  acc=0.752809, mean=0.779331, se=0.032977\n",
      "CV 10:  acc=0.853933, mean=0.786792, se=0.039028\n",
      "==============\n",
      "Mean accuracy:0.786792\n",
      "SE: 0.039028     (Standard error)\n",
      "\n",
      "CrossValidation with 10 folds\n",
      "CV  1:  acc=0.755556, mean=0.755556, se=NaN\n",
      "CV  2:  acc=0.786517, mean=0.771036, se=0.021893\n",
      "CV  3:  acc=0.764045, mean=0.768706, se=0.015998\n",
      "CV  4:  acc=0.752809, mean=0.764732, se=0.015291\n",
      "CV  5:  acc=0.820225, mean=0.775830, se=0.028129\n",
      "CV  6:  acc=0.831461, mean=0.785102, se=0.033894\n",
      "CV  7:  acc=0.831461, mean=0.791725, se=0.035558\n",
      "CV  8:  acc=0.797753, mean=0.792478, se=0.032989\n",
      "CV  9:  acc=0.820225, mean=0.795561, se=0.032215\n",
      "CV 10:  acc=0.808989, mean=0.796904, se=0.030668\n",
      "==============\n",
      "Mean accuracy:0.796904\n",
      "SE: 0.030668     (Standard error)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7969038701622971"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CTree tree = CTree.newCART();\n",
    "CEvaluation.cv(train.mapVars(\"Survived,Sex,Pclass,Embarked\"),\"Survived\", tree, 10);\n",
    "CEvaluation.cv(train.mapVars(\"Survived,Sex,Pclass,Embarked,Age,Fare,SibSp,Parch\"),\"Survived\", tree, 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the 10-crossfold estimator of the accuracy has dropped with a large quantity. What happens? We can have an idea if we take a look at the learned tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTree model\n",
      "================\n",
      "\n",
      "Description:\n",
      "CTree {varSelector=VarSelector[ALL];\n",
      "minCount=1;\n",
      "maxDepth=-1;\n",
      "tests=INT:NumericBinary,NOMINAL:NominalBinary,BINARY:BinaryBinary,DOUBLE:NumericBinary;\n",
      "func=GiniGain;\n",
      "split=ToAllWeighted;\n",
      "}\n",
      "\n",
      "Capabilities:\n",
      "types inputs/targets: BINARY,INT,NOMINAL,DOUBLE/NOMINAL\n",
      "counts inputs/targets: [1,1000000] / [1,1]\n",
      "missing inputs/targets: true/false\n",
      "\n",
      "Learned model:\n",
      "input vars: \n",
      "0.      Sex : NOMINAL  | 4.     Fare : DOUBLE   | \n",
      "1.   Pclass : NOMINAL  | 5.    SibSp : DOUBLE   | \n",
      "2. Embarked : NOMINAL  | 6.    Parch : DOUBLE   | \n",
      "3.      Age : DOUBLE   | \n",
      "\n",
      "target vars:\n",
      "> Survived : NOMINAL [?,0,1]\n",
      "\n",
      "\n",
      "total number of nodes: 509\n",
      "total number of leaves: 255\n",
      "description:\n",
      "split, n/err, classes (densities) [* if is leaf / purity if not]\n",
      "\n",
      "|- 0. root    891/342 0 (0.616 0.384 ) [0.5731486]\n",
      "|   |- 1. Age <= 37.5    703/271 0 (0.608 0.392 ) [0.6304902]\n",
      "|   |   |- 3. Age <= 27.5    514/193 0 (0.606 0.394 ) [0.11952]\n",
      "|   |   |   |- 7. Sex = 'female'    186/55 1 (0.291 0.709 ) [0.0265208]\n",
      "|   |   |   |   |- 15. SibSp <= 2.5    167/40 1 (0.24 0.76 ) [0.021889]\n",
      "|   |   |   |   |   |- 31. Fare <= 18.375    94/32 1 (0.347 0.653 ) [0.0134056]\n",
      "|   |   |   |   |   |   |- 57. Fare <= 17.25    92/30 1 (0.329 0.671 ) [0]\n",
      "|   |   |   |   |   |   |   |- 99. Fare <= 6.9875    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |- 100. Fare > 6.9875    91/29 1 (0.32 0.68 ) [0.0153622]\n",
      "|   |   |   |   |   |   |   |   |- 149. Fare <= 7.7625    26/5 1 (0.16 0.84 ) [0.0142375]\n",
      "|   |   |   |   |   |   |   |   |   |- 191. Fare <= 7.5229    6/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 192. Fare > 7.5229    20/5 1 (0.22 0.78 ) [0.0025643]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 239. Fare <= 7.6396    3/2 1 (0.486 0.514 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 287. Fare <= 7.5896    2/1 1 (0.321 0.679 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 288. Fare > 7.5896    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 240. Fare > 7.6396    17/3 1 (0.174 0.826 ) [0.0171058]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 289. Fare <= 7.74375    4/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 290. Fare > 7.74375    13/3 1 (0.236 0.764 ) *\n",
      "|   |   |   |   |   |   |   |   |- 150. Fare > 7.7625    65/24 1 (0.372 0.628 ) [0.029538]\n",
      "|   |   |   |   |   |   |   |   |   |- 193. Fare <= 10.48125    28/14 0 (0.522 0.478 ) [0.0561556]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 241. Fare <= 8.0396    17/5 1 (0.348 0.652 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 291. Fare <= 7.78125    3/1 0 (0.667 0.333 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 292. Fare > 7.78125    14/3 1 (0.264 0.736 ) [0.012641]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 335. Fare <= 7.9021    9/1 1 (0.157 0.843 ) [0.0086181]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 375. Fare <= 7.8667    5/1 1 (0.254 0.746 ) [0.0174789]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 415. Fare <= 7.8417    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 416. Fare > 7.8417    3/1 1 (0.333 0.667 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 376. Fare > 7.8667    4/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 336. Fare > 7.9021    5/2 1 (0.4 0.6 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 377. Fare <= 7.9771    4/2 1 (0.5 0.5 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 378. Fare > 7.9771    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 242. Fare > 8.0396    11/2 0 (0.788 0.212 ) [0.0311425]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 293. Fare <= 9.1    5/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 294. Fare > 9.1    6/2 0 (0.667 0.333 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 337. Fare <= 9.5875    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 338. Fare > 9.5875    5/1 0 (0.8 0.2 ) [0.0533333]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 379. Fare <= 9.8396    3/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 380. Fare > 9.8396    2/1 0 (0.5 0.5 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 417. Fare <= 10.1521    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 418. Fare > 10.1521    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 194. Fare > 10.48125    37/10 1 (0.261 0.739 ) [0.0406721]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 243. Fare <= 14.15625    18/2 1 (0.114 0.886 ) [0.0054325]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 295. Fare <= 10.825    4/1 1 (0.25 0.75 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 339. Fare <= 10.50835    3/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 340. Fare > 10.50835    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 296. Fare > 10.825    14/1 1 (0.074 0.926 ) [0.0075374]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 341. Fare <= 12.7375    8/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 342. Fare > 12.7375    6/1 1 (0.167 0.833 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 381. Fare <= 13.20835    3/1 1 (0.333 0.667 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 382. Fare > 13.20835    3/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 244. Fare > 14.15625    19/8 1 (0.434 0.566 ) [0.1328966]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 297. Fare <= 15.3729    9/2 0 (0.73 0.27 ) [0.018183]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 343. Fare <= 14.8729    7/2 0 (0.664 0.336 ) [0.0065955]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 383. Fare <= 14.47915    6/1 0 (0.798 0.202 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 419. Fare <= 14.45625    4/1 0 (0.712 0.288 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 420. Fare > 14.45625    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 384. Fare > 14.47915    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 344. Fare > 14.8729    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 298. Fare > 15.3729    10/1 1 (0.136 0.864 ) [0.0180424]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 345. Fare <= 15.975    5/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 346. Fare > 15.975    5/1 1 (0.254 0.746 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 385. Fare <= 16.4    3/2 0 (0.514 0.486 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 386. Fare > 16.4    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |- 58. Fare > 17.25    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |- 32. Fare > 18.375    73/8 1 (0.115 0.885 ) [0.0062409]\n",
      "|   |   |   |   |   |   |- 59. Fare <= 149.0354    66/6 1 (0.094 0.906 ) [0.0072461]\n",
      "|   |   |   |   |   |   |   |- 101. Fare <= 36.6875    35/6 1 (0.172 0.828 ) [0.0399907]\n",
      "|   |   |   |   |   |   |   |   |- 151. Fare <= 33.6875    33/4 1 (0.116 0.884 ) [0.0076437]\n",
      "|   |   |   |   |   |   |   |   |   |- 195. Fare <= 26.125    22/4 1 (0.179 0.821 ) [0.0223973]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 245. Fare <= 20.25    6/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 246. Fare > 20.25    16/4 1 (0.26 0.74 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 299. Fare <= 21.5125    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 300. Fare > 21.5125    15/3 1 (0.2 0.8 ) [0.0374904]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 347. Fare <= 23.35    6/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 348. Fare > 23.35    9/3 1 (0.311 0.689 ) [0.0599432]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 387. Fare <= 25.075    3/1 0 (0.757 0.243 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 421. Fare <= 23.8    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 422. Fare > 23.8    2/1 0 (0.679 0.321 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 388. Fare > 25.075    6/1 1 (0.167 0.833 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 196. Fare > 26.125    11/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |- 152. Fare > 33.6875    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |- 102. Fare > 36.6875    31/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |- 60. Fare > 149.0354    7/2 1 (0.286 0.714 ) [0]\n",
      "|   |   |   |   |   |   |   |- 103. Fare <= 181.44375    3/1 0 (0.667 0.333 ) *\n",
      "|   |   |   |   |   |   |   |- 104. Fare > 181.44375    4/0 1 (0 1 ) *\n",
      "|   |   |   |   |- 16. SibSp > 2.5    19/4 0 (0.747 0.253 ) [0.1615179]\n",
      "|   |   |   |   |   |- 33. Pclass = '1'    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |- 34. Pclass != '1'    17/2 0 (0.855 0.145 ) [0]\n",
      "|   |   |   |   |   |   |- 61. Fare <= 14.5    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |- 62. Fare > 14.5    16/1 0 (0.922 0.078 ) [0.0088706]\n",
      "|   |   |   |   |   |   |   |- 105. Fare <= 31.33125    11/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |- 106. Fare > 31.33125    5/1 0 (0.707 0.293 ) [0]\n",
      "|   |   |   |   |   |   |   |   |- 153. Fare <= 39.14375    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |- 154. Fare > 39.14375    4/0 0 (1 0 ) *\n",
      "|   |   |   |- 8. Sex != 'female'    328/62 0 (0.796 0.204 ) [0.0219003]\n",
      "|   |   |   |   |- 17. Fare <= 15.1729    216/25 0 (0.873 0.127 ) [0.0009955]\n",
      "|   |   |   |   |   |- 35. Fare <= 7.9104    125/11 0 (0.899 0.101 ) [0.0054684]\n",
      "|   |   |   |   |   |   |- 63. Fare <= 7.7979    90/11 0 (0.859 0.141 ) [0.0077829]\n",
      "|   |   |   |   |   |   |   |- 107. Fare <= 7.7854    85/9 0 (0.88 0.12 ) [0.0030538]\n",
      "|   |   |   |   |   |   |   |   |- 155. Fare <= 7.2396    43/6 0 (0.826 0.174 ) [0.0055094]\n",
      "|   |   |   |   |   |   |   |   |   |- 197. Fare <= 7.13335    24/2 0 (0.89 0.11 ) [0.0037155]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 247. Fare <= 7.0125    16/2 0 (0.813 0.187 ) [0.0773044]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 301. Fare <= 6.9625    15/1 0 (0.897 0.103 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 349. Fare <= 2.00625    10/1 0 (0.827 0.173 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 350. Fare > 2.00625    5/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 302. Fare > 6.9625    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 248. Fare > 7.0125    8/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 198. Fare > 7.13335    19/4 0 (0.737 0.263 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 249. Fare <= 7.18335    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 250. Fare > 7.18335    18/3 0 (0.797 0.203 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 303. Fare <= 7.2271    7/1 0 (0.771 0.229 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 304. Fare > 7.2271    11/2 0 (0.812 0.188 ) *\n",
      "|   |   |   |   |   |   |   |   |- 156. Fare > 7.2396    42/3 0 (0.936 0.064 ) [0.0094103]\n",
      "|   |   |   |   |   |   |   |   |   |- 199. Fare <= 7.7625    33/1 0 (0.979 0.021 ) [0.001876]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 251. Fare <= 7.74585    21/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 252. Fare > 7.74585    12/1 0 (0.924 0.076 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 200. Fare > 7.7625    9/2 0 (0.815 0.185 ) *\n",
      "|   |   |   |   |   |   |   |- 108. Fare > 7.7854    5/2 0 (0.6 0.4 ) *\n",
      "|   |   |   |   |   |   |- 64. Fare > 7.7979    35/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |- 36. Fare > 7.9104    91/14 0 (0.841 0.159 ) [0.001715]\n",
      "|   |   |   |   |   |   |- 65. Fare <= 12.7375    68/11 0 (0.823 0.177 ) [0.0169745]\n",
      "|   |   |   |   |   |   |   |- 109. Fare <= 11.9875    67/10 0 (0.837 0.163 ) [0.003925]\n",
      "|   |   |   |   |   |   |   |   |- 157. Fare <= 10.81665    61/8 0 (0.856 0.144 ) [0.0067363]\n",
      "|   |   |   |   |   |   |   |   |   |- 201. Fare <= 8.6875    43/7 0 (0.816 0.184 ) [0.0011045]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 253. Fare <= 8.4875    34/5 0 (0.832 0.168 ) [0.0023753]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 305. Fare <= 8.1354    30/5 0 (0.807 0.193 ) [0.0013444]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 351. Fare <= 8.08125    29/4 0 (0.824 0.176 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 389. Fare <= 7.9875    4/1 0 (0.75 0.25 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 390. Fare > 7.9875    25/3 0 (0.839 0.161 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 352. Fare > 8.08125    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 306. Fare > 8.1354    4/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 254. Fare > 8.4875    9/2 0 (0.764 0.236 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 307. Fare <= 8.5896    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 308. Fare > 8.5896    8/1 0 (0.866 0.134 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 202. Fare > 8.6875    18/1 0 (0.941 0.059 ) [0.009896]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 255. Fare <= 10.3354    12/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 256. Fare > 10.3354    6/1 0 (0.833 0.167 ) *\n",
      "|   |   |   |   |   |   |   |   |- 158. Fare > 10.81665    6/2 0 (0.667 0.333 ) [0.1777778]\n",
      "|   |   |   |   |   |   |   |   |   |- 203. Fare <= 11.37085    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 204. Fare > 11.37085    4/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |- 110. Fare > 11.9875    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |- 66. Fare > 12.7375    23/3 0 (0.899 0.101 ) [0]\n",
      "|   |   |   |   |   |   |   |- 111. Fare <= 13.43125    12/1 0 (0.959 0.041 ) *\n",
      "|   |   |   |   |   |   |   |- 112. Fare > 13.43125    11/2 0 (0.812 0.188 ) [0]\n",
      "|   |   |   |   |   |   |   |   |- 159. Fare <= 14.15835    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |- 160. Fare > 14.15835    10/1 0 (0.864 0.136 ) [0.0137769]\n",
      "|   |   |   |   |   |   |   |   |   |- 205. Fare <= 14.47915    3/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 206. Fare > 14.47915    7/1 0 (0.795 0.205 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 257. Fare <= 14.7729    4/1 0 (0.66 0.34 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 258. Fare > 14.7729    3/0 0 (1 0 ) *\n",
      "|   |   |   |   |- 18. Fare > 15.1729    112/37 0 (0.651 0.349 ) [0.0088957]\n",
      "|   |   |   |   |   |- 37. Fare <= 16    8/3 1 (0.264 0.736 ) [0.0298367]\n",
      "|   |   |   |   |   |   |- 67. Fare <= 15.62085    5/2 0 (0.6 0.4 ) [0]\n",
      "|   |   |   |   |   |   |   |- 113. Fare <= 15.3729    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |- 114. Fare > 15.3729    3/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |- 68. Fare > 15.62085    3/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |- 38. Fare > 16    104/32 0 (0.675 0.325 ) [0.0152534]\n",
      "|   |   |   |   |   |   |- 69. SibSp <= 1.5    72/29 0 (0.558 0.442 ) [0.0176639]\n",
      "|   |   |   |   |   |   |   |- 115. Fare <= 18.275    4/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |- 116. Fare > 18.275    68/29 0 (0.529 0.471 ) [0.0216471]\n",
      "|   |   |   |   |   |   |   |   |- 161. Fare <= 181.525    64/29 0 (0.502 0.498 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |- 207. Fare <= 18.76875    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 208. Fare > 18.76875    62/27 0 (0.522 0.478 ) [0.0207878]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 259. Fare <= 28.3604    23/6 0 (0.682 0.318 ) [0.0108676]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 309. Fare <= 27.1354    21/6 0 (0.663 0.337 ) [0.0074791]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 353. Fare <= 20.3875    6/1 0 (0.798 0.202 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 391. Fare <= 19.3771    2/1 1 (0.5 0.5 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 392. Fare > 19.3771    4/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 354. Fare > 20.3875    15/5 0 (0.604 0.396 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 393. Fare <= 20.55    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 394. Fare > 20.55    14/4 0 (0.663 0.337 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 423. Fare <= 22.0125    2/1 0 (0.5 0.5 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 424. Fare > 22.0125    12/3 0 (0.702 0.298 ) [0.036435]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 445. Fare <= 25.9625    4/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 446. Fare > 25.9625    8/3 0 (0.615 0.385 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 463. Fare <= 26.275    6/2 0 (0.635 0.365 ) *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 464. Fare > 26.275    2/1 1 (0.5 0.5 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 310. Fare > 27.1354    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 260. Fare > 28.3604    39/18 1 (0.436 0.564 ) [0.0390194]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 311. Fare <= 30.5979    6/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 312. Fare > 30.5979    33/15 0 (0.507 0.493 ) [0.0374853]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 355. Fare <= 35.25    4/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 356. Fare > 35.25    29/14 1 (0.459 0.541 ) [0.0162161]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 395. Fare <= 80.52915    22/10 0 (0.527 0.473 ) [0.0276216]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 425. Fare <= 68.42915    17/8 1 (0.42 0.58 ) [0.0729198]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 447. Fare <= 54.27085    11/4 0 (0.585 0.415 ) [0.0409115]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 465. Fare <= 38.3021    4/1 1 (0.288 0.712 ) [0.0261026]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 483. Fare <= 36.8771    3/1 1 (0.405 0.595 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 499. Fare <= 36.125    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 500. Fare > 36.125    2/1 0 (0.5 0.5 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 484. Fare > 36.8771    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 466. Fare > 38.3021    7/1 0 (0.795 0.205 ) [0.0818126]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 485. Fare <= 52.55    5/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 486. Fare > 52.55    2/1 1 (0.5 0.5 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 448. Fare > 54.27085    6/1 1 (0.107 0.893 ) [0.0066885]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 467. Fare <= 59.92705    5/1 1 (0.138 0.862 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 487. Fare <= 55.96875    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 488. Fare > 55.96875    4/1 1 (0.195 0.805 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 468. Fare > 59.92705    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 426. Fare > 68.42915    5/1 0 (0.8 0.2 ) [0.0533333]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 449. Fare <= 77.00835    3/1 0 (0.667 0.333 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 469. Fare <= 75.1146    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 470. Fare > 75.1146    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 450. Fare > 77.00835    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 396. Fare > 80.52915    7/2 1 (0.286 0.714 ) [0.0272109]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 427. Fare <= 99.9896    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 428. Fare > 99.9896    5/2 1 (0.4 0.6 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 451. Fare <= 109.89165    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 452. Fare > 109.89165    4/1 1 (0.25 0.75 ) [0.0416667]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 471. Fare <= 127.81665    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 472. Fare > 127.81665    2/1 1 (0.5 0.5 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 489. Fare <= 143.59165    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 490. Fare > 143.59165    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |- 162. Fare > 181.525    4/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |- 70. SibSp > 1.5    32/3 0 (0.913 0.087 ) [0.0028599]\n",
      "|   |   |   |   |   |   |   |- 117. Fare <= 39.34375    16/3 0 (0.829 0.171 ) [0.0828944]\n",
      "|   |   |   |   |   |   |   |   |- 163. Fare <= 31.33125    13/1 0 (0.959 0.041 ) [0.0009449]\n",
      "|   |   |   |   |   |   |   |   |   |- 209. Fare <= 23.7    4/1 0 (0.84 0.16 ) [0.1089163]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 261. Fare <= 22.4646    3/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 262. Fare > 22.4646    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 210. Fare > 23.7    9/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |- 164. Fare > 31.33125    3/1 1 (0.333 0.667 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |- 211. Fare <= 35.19375    2/1 1 (0.5 0.5 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 212. Fare > 35.19375    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |- 118. Fare > 39.34375    16/0 0 (1 0 ) *\n",
      "|   |   |- 4. Age > 27.5    366/130 0 (0.611 0.389 ) [0.1628202]\n",
      "|   |   |   |- 9. Sex = 'female'    116/28 1 (0.201 0.799 ) [0.0497845]\n",
      "|   |   |   |   |- 19. Fare <= 10.48125    30/17 0 (0.6 0.4 ) [0.0738233]\n",
      "|   |   |   |   |   |- 39. Fare <= 7.8875    22/6 1 (0.419 0.581 ) [0.0275257]\n",
      "|   |   |   |   |   |   |- 71. Fare <= 7.8667    19/6 1 (0.471 0.529 ) [0.049508]\n",
      "|   |   |   |   |   |   |   |- 119. Fare <= 7.8417    18/5 1 (0.374 0.626 ) [0.0141699]\n",
      "|   |   |   |   |   |   |   |   |- 165. Fare <= 7.3896    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |- 166. Fare > 7.3896    16/5 1 (0.414 0.586 ) [0.0386055]\n",
      "|   |   |   |   |   |   |   |   |   |- 213. Fare <= 7.68125    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 214. Fare > 7.68125    14/3 1 (0.344 0.656 ) [0.0150327]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 263. Fare <= 7.74375    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 264. Fare > 7.74375    12/3 1 (0.391 0.609 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 313. Fare <= 7.76875    10/3 1 (0.452 0.548 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 314. Fare > 7.76875    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |- 120. Fare > 7.8417    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |- 72. Fare > 7.8667    3/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |- 40. Fare > 7.8875    8/1 0 (0.827 0.173 ) [0.0267206]\n",
      "|   |   |   |   |   |   |- 73. Fare <= 8.6729    5/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |- 74. Fare > 8.6729    3/1 0 (0.667 0.333 ) [0]\n",
      "|   |   |   |   |   |   |   |- 121. Fare <= 9.1354    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |- 122. Fare > 9.1354    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |- 20. Fare > 10.48125    86/15 1 (0.12 0.88 ) [0.0139009]\n",
      "|   |   |   |   |   |- 41. SibSp <= 5.5    83/12 1 (0.109 0.891 ) [0.0149254]\n",
      "|   |   |   |   |   |   |- 75. Fare <= 25.73335    44/12 1 (0.218 0.782 ) [0.0189955]\n",
      "|   |   |   |   |   |   |   |- 123. Fare <= 13.7    14/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |- 124. Fare > 13.7    30/12 1 (0.376 0.624 ) [0.0580713]\n",
      "|   |   |   |   |   |   |   |   |- 167. Fare <= 14.85205    3/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |- 168. Fare > 14.85205    27/9 1 (0.319 0.681 ) [0.0449441]\n",
      "|   |   |   |   |   |   |   |   |   |- 215. Fare <= 24.075    22/5 1 (0.241 0.759 ) [0.0157085]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 265. Fare <= 19.125    11/3 1 (0.344 0.656 ) [0.0212841]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 315. Fare <= 17.7    10/2 1 (0.226 0.774 ) [0.0001693]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 357. Fare <= 15.675    6/2 1 (0.413 0.587 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 397. Fare <= 15.3729    2/1 1 (0.209 0.791 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 398. Fare > 15.3729    4/3 0 (0.557 0.443 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 358. Fare > 15.675    4/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 316. Fare > 17.7    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 266. Fare > 19.125    11/2 1 (0.157 0.843 ) [0.0162595]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 317. Fare <= 21.0375    4/0 1 (0 1 ) *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   |   |   |   |   |   |   |   |   |   |   |- 318. Fare > 21.0375    7/2 1 (0.312 0.688 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 359. Fare <= 21.71665    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 360. Fare > 21.71665    6/1 1 (0.087 0.913 ) [0.0149778]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 399. Fare <= 23.35    4/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 400. Fare > 23.35    2/1 1 (0.209 0.791 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 429. Fare <= 23.725    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 430. Fare > 23.725    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 216. Fare > 24.075    5/1 0 (0.871 0.129 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 267. Fare <= 24.80835    2/1 0 (0.791 0.209 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 268. Fare > 24.80835    3/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |- 76. Fare > 25.73335    39/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |- 42. SibSp > 5.5    3/0 0 (1 0 ) *\n",
      "|   |   |   |- 10. Sex != 'female'    250/42 0 (0.81 0.19 ) [0.0351861]\n",
      "|   |   |   |   |- 21. Fare <= 26.26875    193/20 0 (0.888 0.112 ) [0.0039067]\n",
      "|   |   |   |   |   |- 43. Fare <= 7.74375    43/1 0 (0.987 0.013 ) [0.0003305]\n",
      "|   |   |   |   |   |   |- 77. Fare <= 7.2271    24/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |- 78. Fare > 7.2271    19/1 0 (0.967 0.033 ) [0]\n",
      "|   |   |   |   |   |   |   |- 125. Fare <= 7.2396    8/1 0 (0.926 0.074 ) *\n",
      "|   |   |   |   |   |   |   |- 126. Fare > 7.2396    11/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |- 44. Fare > 7.74375    150/19 0 (0.868 0.132 ) [0.002296]\n",
      "|   |   |   |   |   |   |- 79. Fare <= 15.3729    118/17 0 (0.847 0.153 ) [0.0054628]\n",
      "|   |   |   |   |   |   |   |- 127. Fare <= 15.1729    116/15 0 (0.853 0.147 ) [0.0010712]\n",
      "|   |   |   |   |   |   |   |   |- 169. Fare <= 14.13125    110/15 0 (0.848 0.152 ) [0.0025787]\n",
      "|   |   |   |   |   |   |   |   |   |- 217. Fare <= 13.68125    109/14 0 (0.851 0.149 ) [0.0010504]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 269. Fare <= 12.9375    98/11 0 (0.863 0.137 ) [0.0021493]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 319. Fare <= 10    89/11 0 (0.841 0.159 ) [0.0065469]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 361. Fare <= 9.49165    83/9 0 (0.864 0.136 ) [0.0027067]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 401. Fare <= 7.9875    54/7 0 (0.831 0.169 ) [0.0011343]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 431. Fare <= 7.9104    48/5 0 (0.868 0.132 ) [0.0037938]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 453. Fare <= 7.8646    23/4 0 (0.801 0.199 ) [0.025507]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 473. Fare <= 7.8417    21/3 0 (0.857 0.143 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 491. Fare <= 7.7625    14/2 0 (0.786 0.214 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 492. Fare > 7.7625    7/1 0 (0.945 0.055 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 501. Fare <= 7.7854    5/1 0 (0.925 0.075 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 502. Fare > 7.7854    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 474. Fare > 7.8417    2/1 0 (0.5 0.5 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 454. Fare > 7.8646    25/1 0 (0.928 0.072 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 475. Fare <= 7.8854    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 476. Fare > 7.8854    24/1 0 (0.923 0.077 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 432. Fare > 7.9104    6/2 0 (0.667 0.333 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 402. Fare > 7.9875    29/2 0 (0.927 0.073 ) [0.0003683]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 433. Fare <= 8.2375    22/2 0 (0.898 0.102 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 455. Fare <= 8.08125    21/1 0 (0.918 0.082 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 456. Fare > 8.08125    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 434. Fare > 8.2375    7/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 362. Fare > 9.49165    6/2 0 (0.62 0.38 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 320. Fare > 10    9/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 270. Fare > 12.9375    11/3 0 (0.779 0.221 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 321. Fare <= 13.25    10/3 0 (0.756 0.244 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 322. Fare > 13.25    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 218. Fare > 13.68125    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |- 170. Fare > 14.13125    6/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |- 128. Fare > 15.1729    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |- 80. Fare > 15.3729    32/2 0 (0.942 0.058 ) [0.008168]\n",
      "|   |   |   |   |   |   |   |- 129. Fare <= 25.9625    25/1 0 (0.983 0.017 ) [0.0013903]\n",
      "|   |   |   |   |   |   |   |   |- 171. Fare <= 22.8875    17/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |- 172. Fare > 22.8875    8/1 0 (0.926 0.074 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |- 219. Fare <= 23.35    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 220. Fare > 23.35    7/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |- 130. Fare > 25.9625    7/1 0 (0.84 0.16 ) [0]\n",
      "|   |   |   |   |   |   |   |   |- 173. Fare <= 26.125    6/1 0 (0.81 0.19 ) *\n",
      "|   |   |   |   |   |   |   |   |- 174. Fare > 26.125    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |- 22. Fare > 26.26875    57/22 0 (0.572 0.428 ) [0.0539538]\n",
      "|   |   |   |   |   |- 45. Fare <= 27.1354    8/1 1 (0.041 0.959 ) [0.0014514]\n",
      "|   |   |   |   |   |   |- 81. Fare <= 26.46875    3/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |- 82. Fare > 26.46875    5/1 1 (0.075 0.925 ) *\n",
      "|   |   |   |   |   |- 46. Fare > 27.1354    49/15 0 (0.677 0.323 ) [0.0491367]\n",
      "|   |   |   |   |   |   |- 83. Fare <= 101.0854    44/12 0 (0.741 0.259 ) [0.0215043]\n",
      "|   |   |   |   |   |   |   |- 131. Fare <= 61.8    36/12 0 (0.687 0.313 ) [0.0617443]\n",
      "|   |   |   |   |   |   |   |   |- 175. Fare <= 52.2771    27/6 0 (0.825 0.175 ) [0.0156066]\n",
      "|   |   |   |   |   |   |   |   |   |- 221. Fare <= 36.2521    18/6 0 (0.731 0.269 ) [0.1225561]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 271. Fare <= 35.25    16/4 0 (0.823 0.177 ) [0.0336904]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 323. Fare <= 30.2854    10/2 0 (0.925 0.075 ) [0.0031117]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 363. Fare <= 28.725    5/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 364. Fare > 28.725    5/2 0 (0.85 0.15 ) [0.0039278]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 403. Fare <= 30.0354    4/2 0 (0.791 0.209 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 435. Fare <= 29.85    2/1 0 (0.791 0.209 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 436. Fare > 29.85    2/1 0 (0.791 0.209 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 404. Fare > 30.0354    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 324. Fare > 30.2854    6/2 0 (0.587 0.413 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 365. Fare <= 30.5979    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 366. Fare > 30.5979    4/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 272. Fare > 35.25    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 222. Fare > 36.2521    9/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |- 176. Fare > 52.2771    9/3 1 (0.333 0.667 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |- 223. Fare <= 52.8271    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 224. Fare > 52.8271    8/3 1 (0.391 0.609 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 273. Fare <= 54.7979    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 274. Fare > 54.7979    7/2 1 (0.264 0.736 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 325. Fare <= 56.7479    6/2 1 (0.333 0.667 ) *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   |   |   |   |   |   |   |   |   |   |   |- 326. Fare > 56.7479    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |- 132. Fare > 61.8    8/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |- 84. Fare > 101.0854    5/2 1 (0.15 0.85 ) [0.0177907]\n",
      "|   |   |   |   |   |   |   |- 133. Fare <= 369.9271    3/2 1 (0.346 0.654 ) [0]\n",
      "|   |   |   |   |   |   |   |   |- 177. Fare <= 170.8896    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |- 178. Fare > 170.8896    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |- 134. Fare > 369.9271    2/0 1 (0 1 ) *\n",
      "|   |- 2. Age > 37.5    365/123 0 (0.639 0.361 ) [0.1561569]\n",
      "|   |   |- 5. Sex = 'female'    118/32 1 (0.247 0.753 ) [0.0575632]\n",
      "|   |   |   |- 11. Fare <= 49.1896    76/29 1 (0.408 0.592 ) [0.0579414]\n",
      "|   |   |   |   |- 23. Pclass = '2'    18/3 1 (0.182 0.818 ) [0.0102224]\n",
      "|   |   |   |   |   |- 47. Fare <= 13.25    7/2 1 (0.319 0.681 ) [0]\n",
      "|   |   |   |   |   |   |- 85. Fare <= 11.425    3/1 1 (0.333 0.667 ) *\n",
      "|   |   |   |   |   |   |- 86. Fare > 11.425    4/1 1 (0.306 0.694 ) [0]\n",
      "|   |   |   |   |   |   |   |- 135. Fare <= 12.675    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |- 136. Fare > 12.675    3/1 1 (0.333 0.667 ) *\n",
      "|   |   |   |   |   |- 48. Fare > 13.25    11/1 1 (0.097 0.903 ) [0.0121258]\n",
      "|   |   |   |   |   |   |- 87. Fare <= 24.5    5/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |- 88. Fare > 24.5    6/1 1 (0.19 0.81 ) [0]\n",
      "|   |   |   |   |   |   |   |- 137. Fare <= 26.125    3/1 1 (0.333 0.667 ) *\n",
      "|   |   |   |   |   |   |   |- 138. Fare > 26.125    3/0 1 (0 1 ) *\n",
      "|   |   |   |   |- 24. Pclass != '2'    58/32 0 (0.536 0.464 ) [0.0279381]\n",
      "|   |   |   |   |   |- 49. Fare <= 27.8104    49/19 1 (0.429 0.571 ) [0.0565474]\n",
      "|   |   |   |   |   |   |- 89. Fare <= 25.69795    45/26 0 (0.534 0.466 ) [0.0256494]\n",
      "|   |   |   |   |   |   |   |- 139. Fare <= 7.9646    21/5 1 (0.328 0.672 ) [0.0197025]\n",
      "|   |   |   |   |   |   |   |   |- 179. Fare <= 7.76875    16/5 1 (0.415 0.585 ) [0.0193373]\n",
      "|   |   |   |   |   |   |   |   |   |- 225. Fare <= 7.3896    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 226. Fare > 7.3896    14/5 1 (0.464 0.536 ) [0.0363419]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 275. Fare <= 7.68125    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 276. Fare > 7.68125    12/3 1 (0.392 0.608 ) [0.0222514]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 327. Fare <= 7.74375    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 328. Fare > 7.74375    10/3 1 (0.453 0.547 ) *\n",
      "|   |   |   |   |   |   |   |   |- 180. Fare > 7.76875    5/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |- 140. Fare > 7.9646    24/10 0 (0.663 0.337 ) [0.0502301]\n",
      "|   |   |   |   |   |   |   |   |- 181. Fare <= 15.3729    10/1 0 (0.821 0.179 ) [0.0303383]\n",
      "|   |   |   |   |   |   |   |   |   |- 227. Fare <= 12.02085    5/1 0 (0.642 0.358 ) [0.1014843]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 277. Fare <= 9.53125    4/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 278. Fare > 9.53125    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 228. Fare > 12.02085    5/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |- 182. Fare > 15.3729    14/5 1 (0.464 0.536 ) [0.0937093]\n",
      "|   |   |   |   |   |   |   |   |   |- 229. Fare <= 18.15625    5/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 230. Fare > 18.15625    9/4 0 (0.661 0.339 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 279. Fare <= 21.2854    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 280. Fare > 21.2854    8/4 1 (0.5 0.5 ) [0.3]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 329. Fare <= 24.80835    5/1 1 (0.2 0.8 ) [0.0533333]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 367. Fare <= 23.35    3/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 368. Fare > 23.35    2/1 1 (0.5 0.5 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 405. Fare <= 23.8    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 406. Fare > 23.8    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 330. Fare > 24.80835    3/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |- 90. Fare > 25.69795    4/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |- 50. Fare > 27.8104    9/2 0 (0.778 0.222 ) [0.0493827]\n",
      "|   |   |   |   |   |   |- 91. Fare <= 31.33125    4/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |- 92. Fare > 31.33125    5/2 0 (0.6 0.4 ) [0.18]\n",
      "|   |   |   |   |   |   |   |- 141. Fare <= 39.64375    3/1 1 (0.333 0.667 ) [0.1111111]\n",
      "|   |   |   |   |   |   |   |   |- 183. Fare <= 36.9875    2/1 0 (0.5 0.5 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |- 231. Fare <= 32.88125    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 232. Fare > 32.88125    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |- 184. Fare > 36.9875    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |- 142. Fare > 39.64375    2/0 0 (1 0 ) *\n",
      "|   |   |   |- 12. Fare > 49.1896    42/3 1 (0.024 0.976 ) [0.0198907]\n",
      "|   |   |   |   |- 25. SibSp <= 5    39/0 1 (0 1 ) *\n",
      "|   |   |   |   |- 26. SibSp > 5    3/0 0 (1 0 ) *\n",
      "|   |   |- 6. Sex != 'female'    247/37 0 (0.838 0.162 ) [0.0196887]\n",
      "|   |   |   |- 13. Fare <= 26.26875    167/14 0 (0.921 0.079 ) [0.0038356]\n",
      "|   |   |   |   |- 27. Fare <= 7.9104    84/3 0 (0.98 0.02 ) [0.0003777]\n",
      "|   |   |   |   |   |- 51. Fare <= 7.2271    28/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |- 52. Fare > 7.2271    56/3 0 (0.967 0.033 ) [0]\n",
      "|   |   |   |   |   |   |- 93. Fare <= 7.2396    6/1 0 (0.833 0.167 ) *\n",
      "|   |   |   |   |   |   |- 94. Fare > 7.2396    50/2 0 (0.976 0.024 ) [0.0004666]\n",
      "|   |   |   |   |   |   |   |- 143. Fare <= 7.8021    31/2 0 (0.964 0.036 ) [0.0019168]\n",
      "|   |   |   |   |   |   |   |   |- 185. Fare <= 7.7625    28/1 0 (0.98 0.02 ) [0.000672]\n",
      "|   |   |   |   |   |   |   |   |   |- 233. Fare <= 7.74375    13/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 234. Fare > 7.74375    15/1 0 (0.962 0.038 ) *\n",
      "|   |   |   |   |   |   |   |   |- 186. Fare > 7.7625    3/1 0 (0.828 0.172 ) *\n",
      "|   |   |   |   |   |   |   |- 144. Fare > 7.8021    19/0 0 (1 0 ) *\n",
      "|   |   |   |   |- 28. Fare > 7.9104    83/11 0 (0.877 0.123 ) [0]\n",
      "|   |   |   |   |   |- 53. Fare <= 7.9875    3/1 1 (0.333 0.667 ) *\n",
      "|   |   |   |   |   |- 54. Fare > 7.9875    80/9 0 (0.909 0.091 ) [0.0043312]\n",
      "|   |   |   |   |   |   |- 95. Fare <= 23.35    65/9 0 (0.885 0.115 ) [0.0039673]\n",
      "|   |   |   |   |   |   |   |- 145. Fare <= 22.4646    64/8 0 (0.891 0.109 ) [0.0019103]\n",
      "|   |   |   |   |   |   |   |   |- 187. Fare <= 15.3729    55/8 0 (0.88 0.12 ) [0.0134471]\n",
      "|   |   |   |   |   |   |   |   |   |- 235. Fare <= 15.1729    53/6 0 (0.893 0.107 ) [0.0028778]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 281. Fare <= 13.93125    44/6 0 (0.874 0.126 ) [0.0042555]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 331. Fare <= 13.68125    43/5 0 (0.881 0.119 ) [0.0026458]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 369. Fare <= 10    28/2 0 (0.918 0.082 ) [0.0010786]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 407. Fare <= 8.25835    20/2 0 (0.879 0.121 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 437. Fare <= 8.08125    19/1 0 (0.902 0.098 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 438. Fare > 8.08125    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 408. Fare > 8.25835    8/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 370. Fare > 10    15/3 0 (0.841 0.159 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 409. Fare <= 11.425    3/1 0 (0.667 0.333 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 410. Fare > 11.425    12/2 0 (0.888 0.112 ) [0.0054322]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 439. Fare <= 13.25    10/2 0 (0.864 0.136 ) [0.0045015]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 457. Fare <= 12.7625    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 458. Fare > 12.7625    8/2 0 (0.826 0.174 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 440. Fare > 13.25    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 332. Fare > 13.68125    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 282. Fare > 13.93125    9/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 236. Fare > 15.1729    2/0 1 (0 1 ) *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   |   |   |   |   |   |   |   |- 188. Fare > 15.3729    9/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |- 146. Fare > 22.4646    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |- 96. Fare > 23.35    15/0 0 (1 0 ) *\n",
      "|   |   |   |- 14. Fare > 26.26875    80/23 0 (0.714 0.286 ) [0.0054363]\n",
      "|   |   |   |   |- 29. Pclass = '2'    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |- 30. Pclass != '2'    78/23 0 (0.704 0.296 ) [0]\n",
      "|   |   |   |   |   |- 55. Fare <= 26.41875    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |- 56. Fare > 26.41875    77/22 0 (0.716 0.284 ) [0.0042251]\n",
      "|   |   |   |   |   |   |- 97. Fare <= 143.55625    73/22 0 (0.703 0.297 ) [0.004899]\n",
      "|   |   |   |   |   |   |   |- 147. Fare <= 123.4625    72/21 0 (0.716 0.284 ) [0.0044632]\n",
      "|   |   |   |   |   |   |   |   |- 189. Fare <= 98.2125    69/21 0 (0.7 0.3 ) [0.0085638]\n",
      "|   |   |   |   |   |   |   |   |   |- 237. Fare <= 86.2896    66/19 0 (0.722 0.278 ) [0.0064486]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 283. Fare <= 79.425    64/19 0 (0.71 0.29 ) [0.0047609]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 333. Fare <= 73.8646    60/17 0 (0.73 0.27 ) [0.0099995]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 371. Fare <= 59.0521    52/17 0 (0.694 0.306 ) [0.0282568]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 411. Fare <= 56.1979    48/14 0 (0.721 0.279 ) [0.0067331]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 441. Fare <= 31.1375    27/10 0 (0.654 0.346 ) [0.0330621]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 459. Fare <= 29.85    18/5 0 (0.754 0.246 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 477. Fare <= 27.1354    11/4 0 (0.657 0.343 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 478. Fare > 27.1354    7/1 0 (0.945 0.055 ) [0.0067467]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 493. Fare <= 29.1    5/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 494. Fare > 29.1    2/1 0 (0.792 0.208 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 460. Fare > 29.85    9/4 1 (0.417 0.583 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 479. Fare <= 30.25    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 480. Fare > 30.25    7/3 0 (0.527 0.473 ) [0.013144]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 495. Fare <= 30.8479    5/2 0 (0.642 0.358 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 503. Fare <= 30.5979    3/1 1 (0.442 0.558 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 504. Fare > 30.5979    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 496. Fare > 30.8479    2/1 1 (0.208 0.792 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 442. Fare > 31.1375    21/4 0 (0.803 0.197 ) [0.0334544]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 461. Fare <= 35.25    6/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 462. Fare > 35.25    15/4 0 (0.712 0.288 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 481. Fare <= 37    3/1 1 (0.442 0.558 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 482. Fare > 37    12/2 0 (0.779 0.221 ) [0.0434782]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 497. Fare <= 51.93125    6/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 498. Fare > 51.93125    6/2 0 (0.62 0.38 ) [0.0176539]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 505. Fare <= 54.2271    5/2 0 (0.531 0.469 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 507. Fare <= 52.2771    4/1 0 (0.694 0.306 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 508. Fare > 52.2771    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 506. Fare > 54.2271    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 412. Fare > 56.1979    4/1 1 (0.147 0.853 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 443. Fare <= 56.7125    3/1 1 (0.333 0.667 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 444. Fare > 56.7125    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 372. Fare > 59.0521    8/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 334. Fare > 73.8646    4/2 1 (0.5 0.5 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 373. Fare <= 77.00835    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 374. Fare > 77.00835    3/1 0 (0.667 0.333 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 413. Fare <= 78.24375    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 414. Fare > 78.24375    2/1 0 (0.5 0.5 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 284. Fare > 79.425    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 238. Fare > 86.2896    3/1 1 (0.333 0.667 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 285. Fare <= 89.5521    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 286. Fare > 89.5521    2/1 1 (0.5 0.5 ) *\n",
      "|   |   |   |   |   |   |   |   |- 190. Fare > 98.2125    3/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |- 148. Fare > 123.4625    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |- 98. Fare > 143.55625    4/0 0 (1 0 ) *\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree.fit(train.mapVars(\"Survived,Sex,Pclass,Embarked,Age,Fare,SibSp,Parch\"), \"Survived\");\n",
    "tree.printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Notice how large is the tree. Basically the tree was full grown and overfit the training data set too much. We can ask ourselves why that happens? Why it happens now, and did not happened when we had fewer inputs? The answer is that it happened also before. But it's consequences were not so drastic.\n",
    "\n",
    "The first tree used for training just $3$ input nominal features. Notice that all three features are nominal. The maximum number of groups which one can form is given by the product of the number of levels for each feature. This total maximal number is $2*3*3=18$. It practically exhausted the discrimination potential of those features. It did overfit in that reduced space of features. When we apply the model to the whole data set, the effect of exhaustion is not seen anymore.\n",
    "\n",
    "The second tree does the same thing, but this time in a richer space, with added input dimensions. Compared with the full feature space, we see the effect.\n",
    "\n",
    "There are two approaches to avoid overfit for a decision tree. The first approach is to stop learning up to the moment when we exhaust the data. The name for this approach is *early stop*. We can do that by specifying some parameters of the tree model:\n",
    "\n",
    "* Set a minimum number of instances for leaf node\n",
    "* Set a maximal depth for the tree\n",
    "* Not implemented yet, but easy to do: complexity threshold, maximal number of nodes in a tree\n",
    "\n",
    "The second approach is to prune the tree. Pruning procedure consists of growing the full tree and later on removing some nodes if they do not provide some type of gain. Currently we implemented only *reduced error pruning strategy*.\n",
    "\n",
    "We will test with 10-fold cross validation an early-stopping strategy to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CrossValidation with 10 folds\n",
      "CV  1:  acc=0.722222, mean=0.722222, se=NaN\n",
      "CV  2:  acc=0.831461, mean=0.776841, se=0.077243\n",
      "CV  3:  acc=0.775281, mean=0.776321, se=0.054627\n",
      "CV  4:  acc=0.808989, mean=0.784488, se=0.047499\n",
      "CV  5:  acc=0.842697, mean=0.796130, se=0.048680\n",
      "CV  6:  acc=0.752809, mean=0.788910, se=0.046996\n",
      "CV  7:  acc=0.741573, mean=0.782147, se=0.046482\n",
      "CV  8:  acc=0.808989, mean=0.785502, se=0.044068\n",
      "CV  9:  acc=0.831461, mean=0.790609, se=0.043977\n",
      "CV 10:  acc=0.842697, mean=0.795818, se=0.044614\n",
      "==============\n",
      "Mean accuracy:0.795818\n",
      "SE: 0.044614     (Standard error)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7958177278401998"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CEvaluation.cv(train.mapVars(\"Survived,Sex,Pclass,Embarked,Age,Fare,SibSp,Parch\"),\n",
    "\"Survived\", tree.withMaxDepth(12).withMinCount(4), 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I tried some values, just to show that we can do something about it, but the progress did not appear. We should try a different approach, and that is an ensemble. Next session contains directions on how to build such an ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. CForest model\n",
    "\n",
    "Random forests are well-known to work well when the irreducible error from the training data is high. This is probably the case of this Titanic data set. We have reasons to believe that this is the situation since it was a tragedy. A lot of random or not-so-expected things happened. That happened despite of the bravery and the sacrifice of the crew and others.\n",
    "\n",
    "Random forests are the invention of [Leo Breiman](https://en.wikipedia.org/wiki/Leo_Breiman). The first design was a joint effort together with [Adele Cutler](http://www.math.usu.edu/adele/). The base of random forests is bagging (or **b**ootstrapp **ag**gregation). On top of that, selecting just a random limited number of variables at each node is the core of the algorithm.\n",
    "\n",
    "We will work with random forests for now. This ensemble is mode robust and is capable of obtaining much better results than a single tree. At the same time we will introduce 10-fold cross validation to check our progress and estimate the error produced.\n",
    "\n",
    "In the beginning we will use 10-fold cross validation for estimating the accuracy on public leader board.\n",
    "\n",
    "We will test first with 10 fold cv the tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CrossValidation with 10 folds\n",
      "CV  1:  acc=0.811111, mean=0.811111, se=NaN\n",
      "CV  2:  acc=0.764045, mean=0.787578, se=0.033281\n",
      "CV  3:  acc=0.831461, mean=0.802206, se=0.034579\n",
      "CV  4:  acc=0.741573, mean=0.787047, se=0.041427\n",
      "CV  5:  acc=0.752809, mean=0.780200, se=0.039008\n",
      "CV  6:  acc=0.752809, mean=0.775635, se=0.036638\n",
      "CV  7:  acc=0.808989, mean=0.780400, se=0.035743\n",
      "CV  8:  acc=0.842697, mean=0.788187, se=0.039751\n",
      "CV  9:  acc=0.786517, mean=0.788001, se=0.037188\n",
      "CV 10:  acc=0.775281, mean=0.786729, se=0.035291\n",
      "==============\n",
      "Mean accuracy:0.786729\n",
      "SE: 0.035291     (Standard error)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.786729088639201"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CEvaluation.cv(train.mapVars(\"Survived,Sex,Pclass,Embarked\"), \"Survived\", CTree.newCART(), 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.1 Our first random forest\n",
    "\n",
    "The name of the random forest implementation is `CForest`. To build a new ensemble of trees, one have to instantiate it in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Classifier rf = CForest.newRF();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of things which can be customized for a random forest. Among them one can change:\n",
    "\n",
    "* Number of trees for classification\n",
    "* Which kind of weak classifier to use (you can customize this customized accordingly, like any other classifier)\n",
    "* Number of threads in pool (if you want to use parallelism)\n",
    "* What to do after each running step\n",
    "\n",
    "Let's build one and use ore new cross validation procedure to estimate it's error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CrossValidation with 10 folds\n",
      "CV  1:  acc=0.733333, mean=0.733333, se=NaN\n",
      "CV  2:  acc=0.808989, mean=0.771161, se=0.053496\n",
      "CV  3:  acc=0.820225, mean=0.787516, se=0.047258\n",
      "CV  4:  acc=0.764045, mean=0.781648, se=0.040331\n",
      "CV  5:  acc=0.831461, mean=0.791610, se=0.041427\n",
      "CV  6:  acc=0.831461, mean=0.798252, se=0.040468\n",
      "CV  7:  acc=0.730337, mean=0.788550, se=0.044985\n",
      "CV  8:  acc=0.820225, mean=0.792509, se=0.043127\n",
      "CV  9:  acc=0.820225, mean=0.795589, se=0.041386\n",
      "CV 10:  acc=0.674157, mean=0.783446, se=0.054745\n",
      "==============\n",
      "Mean accuracy:0.783446\n",
      "SE: 0.054745     (Standard error)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7834456928838951"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomSource.setSeed(123);\n",
    "Frame tr = train.mapVars(\"Survived,Sex,Pclass,Embarked\");\n",
    "CForest rf = CForest.newRF().withRuns(100);\n",
    "CEvaluation.cv(tr, \"Survived\", rf, 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, an identical output. This is due to the fact that our variables are already exhausted by the tree. It looks like an underfit. If one consider bias variance trade off, one can see this as high bias. We need to enrich our feature space to improve our performance.\n",
    "\n",
    "Let's be direct and test what would happen if we would use all our directly usable features? This time we will fit also the training data set, to see the distribution of the training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CrossValidation with 10 folds\n",
      "CV  1:  acc=0.855556, mean=0.855556, se=NaN\n",
      "CV  2:  acc=0.808989, mean=0.832272, se=0.032928\n",
      "CV  3:  acc=0.842697, mean=0.835747, se=0.024049\n",
      "CV  4:  acc=0.808989, mean=0.829057, se=0.023760\n",
      "CV  5:  acc=0.842697, mean=0.831785, se=0.021462\n",
      "CV  6:  acc=0.831461, mean=0.831731, se=0.019197\n",
      "CV  7:  acc=0.797753, mean=0.826877, se=0.021726\n",
      "CV  8:  acc=0.820225, mean=0.826046, se=0.020252\n",
      "CV  9:  acc=0.831461, mean=0.826647, se=0.019030\n",
      "CV 10:  acc=0.696629, mean=0.813645, se=0.044859\n",
      "==============\n",
      "Mean accuracy:0.813645\n",
      "SE: 0.044859     (Standard error)\n",
      "> Confusion\n",
      "\n",
      "Ac\\Pr |    0    1 | total \n",
      "----- |    -    - | ----- \n",
      "    0 | >528   21 |   549 \n",
      "    1 |   69 >273 |   342 \n",
      "----- |    -    - | ----- \n",
      "total |  597  294 |   891 \n",
      "\n",
      "\n",
      "Complete cases 891 from 891\n",
      "Acc: 0.8989899         (Accuracy )\n",
      "F1:  0.921466         (F1 score / F-measure)\n",
      "MCC: 0.7860473         (Matthew correlation coefficient)\n",
      "Pre: 0.8844221         (Precision)\n",
      "Rec: 0.9617486         (Recall)\n",
      "G:   0.9222753         (G-measure)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RandomSource.setSeed(123);\n",
    "Frame tr = train.mapVars(\"Survived,Sex,Pclass,Embarked,Age,Fare,SibSp,Parch\");\n",
    "CForest rf = CForest.newRF().withRuns(100);\n",
    "CEvaluation.cv(tr, \"Survived\", rf, 10);\n",
    "\n",
    "rf.fit(tr, \"Survived\");\n",
    "ClassResult fit = rf.predict(test);\n",
    "new Confusion(tr.rvar(\"Survived\"), rf.predict(tr).firstClasses()).printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we have a good example of overfit. Why is that? Look at the confusion matrix on the training set. We fit too well the training data. This data set is well known for its high irreducible error. And there is an explanation for that. During the tragic event a lot of exceptional things happened. For example I read somewhere that an old lady which had a dog was not allowed to embark with her pet due to regulations. As a consequence she decided to not leave it and she chose to die with him. It's close to impossible to learn those kind of things, even if the information would be available. \n",
    "\n",
    "We should reduce the error somehow. We can try to decrease the overfit by adding more learners. Let's see if that would be enough for our purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CrossValidation with 10 folds\n",
      "CV  1:  acc=0.844444, mean=0.844444, se=NaN\n",
      "CV  2:  acc=0.820225, mean=0.832335, se=0.017126\n",
      "CV  3:  acc=0.820225, mean=0.828298, se=0.013983\n",
      "CV  4:  acc=0.820225, mean=0.826280, se=0.012110\n",
      "CV  5:  acc=0.808989, mean=0.822821, se=0.013030\n",
      "CV  6:  acc=0.842697, mean=0.826134, se=0.014201\n",
      "CV  7:  acc=0.797753, mean=0.822080, se=0.016826\n",
      "CV  8:  acc=0.876404, mean=0.828870, se=0.024730\n",
      "CV  9:  acc=0.820225, mean=0.827910, se=0.023312\n",
      "CV 10:  acc=0.685393, mean=0.813658, se=0.050141\n",
      "==============\n",
      "Mean accuracy:0.813658\n",
      "SE: 0.050141     (Standard error)\n",
      "> Confusion\n",
      "\n",
      "Ac\\Pr |    0    1 | total \n",
      "----- |    -    - | ----- \n",
      "    0 | >530   19 |   549 \n",
      "    1 |   75 >267 |   342 \n",
      "----- |    -    - | ----- \n",
      "total |  605  286 |   891 \n",
      "\n",
      "\n",
      "Complete cases 891 from 891\n",
      "Acc: 0.8945006         (Accuracy )\n",
      "F1:  0.9185442         (F1 score / F-measure)\n",
      "MCC: 0.7771981         (Matthew correlation coefficient)\n",
      "Pre: 0.8760331         (Precision)\n",
      "Rec: 0.9653916         (Recall)\n",
      "G:   0.9196276         (G-measure)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RandomSource.setSeed(123);\n",
    "Frame tr = train.mapVars(\"Survived,Sex,Pclass,Embarked,Age,Fare,SibSp,Parch\");\n",
    "CForest rf = CForest.newRF().withRuns(500);\n",
    "CEvaluation.cv(tr, \"Survived\", rf, 10);\n",
    "\n",
    "rf.fit(tr, \"Survived\");\n",
    "ClassResult fit = rf.predict(test);\n",
    "new Confusion(tr.rvar(\"Survived\"), rf.predict(tr).firstClasses()).printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is slightly better than before. But the difference does not look significantly better than previous. We will use a simple pre-pruning strategy is to limit the number instances in leaf nodes. We set the minimum count to $3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CrossValidation with 10 folds\n",
      "CV  1:  acc=0.822222, mean=0.822222, se=NaN\n",
      "CV  2:  acc=0.820225, mean=0.821223, se=0.001412\n",
      "CV  3:  acc=0.853933, mean=0.832127, se=0.018911\n",
      "CV  4:  acc=0.820225, mean=0.829151, se=0.016548\n",
      "CV  5:  acc=0.786517, mean=0.820624, se=0.023852\n",
      "CV  6:  acc=0.853933, mean=0.826176, se=0.025299\n",
      "CV  7:  acc=0.775281, mean=0.818905, se=0.030057\n",
      "CV  8:  acc=0.820225, mean=0.819070, se=0.027831\n",
      "CV  9:  acc=0.853933, mean=0.822944, se=0.028509\n",
      "CV 10:  acc=0.696629, mean=0.810312, se=0.048146\n",
      "==============\n",
      "Mean accuracy:0.810312\n",
      "SE: 0.048146     (Standard error)\n",
      "> Confusion\n",
      "\n",
      "Ac\\Pr |    0    1 | total \n",
      "----- |    -    - | ----- \n",
      "    0 | >519   30 |   549 \n",
      "    1 |   70 >272 |   342 \n",
      "----- |    -    - | ----- \n",
      "total |  589  302 |   891 \n",
      "\n",
      "\n",
      "Complete cases 891 from 891\n",
      "Acc: 0.8877666         (Accuracy )\n",
      "F1:  0.9121265         (F1 score / F-measure)\n",
      "MCC: 0.7609688         (Matthew correlation coefficient)\n",
      "Pre: 0.8811545         (Precision)\n",
      "Rec: 0.9453552         (Recall)\n",
      "G:   0.9126905         (G-measure)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RandomSource.setSeed(123);\n",
    "Frame tr = train.mapVars(\"Survived,Sex,Pclass,Embarked,Age,Fare,SibSp,Parch\");\n",
    "CForest rf = CForest.newRF()\n",
    ".withClassifier(CTree.newCART().withMinCount(3))\n",
    ".withRuns(100);\n",
    "CEvaluation.cv(tr, \"Survived\", rf, 10);\n",
    "\n",
    "rf.fit(tr, \"Survived\");\n",
    "ClassResult fit = rf.predict(test);\n",
    "new Confusion(tr.rvar(\"Survived\"), rf.predict(tr).firstClasses()).printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we changed the classifier used by `CForest`. This is the same classifier used by default by random forest. We do this because we customized the classifier by changing the min count parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That had indeed some effect. However after submitting to competition we did not saw any improvement. We should look forward to engineer a little bit our features for further improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Feature engineering\n",
    "\n",
    "### 5.1 Title feature\n",
    "\n",
    "It is clear that we can't use directly the `\"Name\"` variable. This is due to the fact that names are almost unique, and that leads to a tiny generalization power. To understand that we should see that even if we learned that a passenger with a given name survived or not. We can't decide if another passenger survived, using only the name of the new passenger.\n",
    "\n",
    "Lets inspect some of the values from `\"Name\"` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VarNominal [name:\"Name\", rowCount:891]\n",
      " row                             value                            \n",
      "  [0]                  \"Braund, Mr. Owen Harris\"                  \n",
      "  [1]    \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\"    \n",
      "  [2]                  \"Heikkinen, Miss. Laina\"                   \n",
      "  [3]       \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\"        \n",
      "  [4]                 \"Allen, Mr. William Henry\"                  \n",
      "  [5]                     \"Moran, Mr. James\"                      \n",
      "  [6]                  \"McCarthy, Mr. Timothy J\"                  \n",
      "  [7]              \"Palsson, Master. Gosta Leonard\"               \n",
      "  [8]     \"Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\"     \n",
      "  [9]            \"Nasser, Mrs. Nicholas (Adele Achem)\"            \n",
      " [10]              \"Sandstrom, Miss. Marguerite Rut\"              \n",
      " [11]                 \"Bonnell, Miss. Elizabeth\"                  \n",
      " [12]              \"Saundercock, Mr. William Henry\"               \n",
      " [13]                \"Andersson, Mr. Anders Johan\"                \n",
      " [14]           \"Vestrom, Miss. Hulda Amanda Adolfina\"            \n",
      " [15]             \"Hewlett, Mrs. (Mary D Kingcome) \"              \n",
      " [16]                   \"Rice, Master. Eugene\"                    \n",
      " [17]               \"Williams, Mr. Charles Eugene\"                \n",
      " [18]  \"Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele)\"  \n",
      " [19]                  \"Masselmani, Mrs. Fatima\"                  \n",
      " [20]                   \"Fynney, Mr. Joseph J\"                    \n",
      " [21]                   \"Beesley, Mr. Lawrence\"                   \n",
      " [22]               \"McGowan, Miss. Anna \"\"Annie\"\"\"               \n",
      " [23]               \"Sloper, Mr. William Thompson\"                \n",
      " [24]               \"Palsson, Miss. Torborg Danira\"               \n",
      " [25] \"Asplund, Mrs. Carl Oscar (Selma Augusta Emilia Johansson)\" \n",
      " [26]                  \"Emir, Mr. Farred Chehab\"                  \n",
      " [27]              \"Fortune, Mr. Charles Alexander\"               \n",
      " [28]              \"O'Dwyer, Miss. Ellen \"\"Nellie\"\"\"              \n",
      " [29]                    \"Todoroff, Mr. Lalio\"                    \n",
      " [30]                 \"Uruchurtu, Don. Manuel E\"                  \n",
      " [31]      \"Spencer, Mrs. William Augustus (Marie Eugenie)\"       \n",
      " [32]                 \"Glynn, Miss. Mary Agatha\"                  \n",
      " [33]                   \"Wheadon, Mr. Edward H\"                   \n",
      " [34]                  \"Meyer, Mr. Edgar Joseph\"                  \n",
      " [35]              \"Holverson, Mr. Alexander Oskar\"               \n",
      " [36]                     \"Mamee, Mr. Hanna\"                      \n",
      " [37]                 \"Cann, Mr. Ernest Charles\"                  \n",
      " [38]            \"Vander Planke, Miss. Augusta Maria\"             \n",
      " [39]                \"Nicola-Yarred, Miss. Jamila\"                \n",
      " [40]      \"Ahlin, Mrs. Johan (Johanna Persdotter Larsson)\"       \n",
      " [41] \"Turpin, Mrs. William John Robert (Dorothy Ann Wonnacott)\"  \n",
      " [42]                    \"Kraeff, Mr. Theodor\"                    \n",
      " [43]         \"Laroche, Miss. Simonne Marie Anne Andree\"          \n",
      " [44]               \"Devaney, Miss. Margaret Delia\"               \n",
      " [45]                 \"Rogers, Mr. William John\"                  \n",
      " [46]                     \"Lennon, Mr. Denis\"                     \n",
      " [47]                 \"O'Driscoll, Miss. Bridget\"                 \n",
      " [48]                    \"Samaan, Mr. Youssef\"                    \n",
      " [49]       \"Arnold-Franchi, Mrs. Josef (Josefine Franchi)\"       \n",
      " [50]                \"Panula, Master. Juha Niilo\"                 \n",
      " [51]               \"Nosworthy, Mr. Richard Cater\"                \n",
      " [52]         \"Harper, Mrs. Henry Sleeper (Myna Haxtun)\"          \n",
      " [53]    \"Faunthorpe, Mrs. Lizzie (Elizabeth Anne Wilkinson)\"     \n",
      " [54]              \"Ostby, Mr. Engelhart Cornelius\"               \n",
      " [55]                     \"Woolner, Mr. Hugh\"                     \n",
      " [56]                     \"Rugg, Miss. Emily\"                     \n",
      " [57]                    \"Novel, Mr. Mansouer\"                    \n",
      " [58]               \"West, Miss. Constance Mirium\"                \n",
      " [59]            \"Goodwin, Master. William Frederick\"             \n",
      " [60]                   \"Sirayanian, Mr. Orsen\"                   \n",
      " [61]                    \"Icard, Miss. Amelie\"                    \n",
      " [62]                \"Harris, Mr. Henry Birkhardt\"                \n",
      " [63]                   \"Skoog, Master. Harald\"                   \n",
      " [64]                   \"Stewart, Mr. Albert A\"                   \n",
      " [65]                 \"Moubarek, Master. Gerios\"                  \n",
      " [66]               \"Nye, Mrs. (Elizabeth Ramell)\"                \n",
      " [67]                 \"Crease, Mr. Ernest James\"                  \n",
      " [68]              \"Andersson, Miss. Erna Alexandra\"              \n",
      " [69]                     \"Kink, Mr. Vincenz\"                     \n",
      " [70]                \"Jenkin, Mr. Stephen Curnow\"                 \n",
      " [71]                \"Goodwin, Miss. Lillian Amy\"                 \n",
      " [72]                   \"Hood, Mr. Ambrose Jr\"                    \n",
      " [73]                \"Chronopoulos, Mr. Apostolos\"                \n",
      " [74]                       \"Bing, Mr. Lee\"                       \n",
      " [75]                  \"Moen, Mr. Sigurd Hansen\"                  \n",
      " [76]                     \"Staneff, Mr. Ivan\"                     \n",
      " [77]                 \"Moutal, Mr. Rahamin Haim\"                  \n",
      " [78]               \"Caldwell, Master. Alden Gates\"               \n",
      " ...                              ...                             \n",
      "[871]     \"Beckwith, Mrs. Richard Leonard (Sallie Monypeny)\"      \n",
      "[872]                 \"Carlsson, Mr. Frans Olof\"                  \n",
      "[873]                \"Vander Cruyssen, Mr. Victor\"                \n",
      "[874]           \"Abelson, Mrs. Samuel (Hannah Wizosky)\"           \n",
      "[875]            \"Najib, Miss. Adele Kiamie \"\"Jane\"\"\"             \n",
      "[876]               \"Gustafsson, Mr. Alfred Ossian\"               \n",
      "[877]                   \"Petroff, Mr. Nedelio\"                    \n",
      "[878]                    \"Laleff, Mr. Kristo\"                     \n",
      "[879]       \"Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)\"       \n",
      "[880]       \"Shelley, Mrs. William (Imanita Parrish Hall)\"        \n",
      "[881]                    \"Markun, Mr. Johann\"                     \n",
      "[882]               \"Dahlberg, Miss. Gerda Ulrika\"                \n",
      "[883]               \"Banfield, Mr. Frederick James\"               \n",
      "[884]                  \"Sutehall, Mr. Henry Jr\"                   \n",
      "[885]           \"Rice, Mrs. William (Margaret Norton)\"            \n",
      "[886]                   \"Montvila, Rev. Juozas\"                   \n",
      "[887]               \"Graham, Miss. Margaret Edith\"                \n",
      "[888]        \"Johnston, Miss. Catherine Helen \"\"Carrie\"\"\"         \n",
      "[889]                   \"Behr, Mr. Karl Howell\"                   \n",
      "[890]                    \"Dooley, Mr. Patrick\"                    \n",
      "                                                                  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.rvar(\"Name\").printContent();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We notice that the names contains title information of the individual. This is valuable, but how can we benefit from that? First of all see that the format of that string is clear: space + title + dot + space. We can try to model a regular expression or we can take a simpler, but manual path. Intuition tells us that there should not be too many keys.\n",
    "\n",
    "We build a set with known keys. After that we filter out names with known titles, and print first twenty of them. We see that we have already `\"Mrs\"` and `\"Mr\"`. Let's find others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Heikkinen, Miss. Laina\"\n",
      "\"Palsson, Master. Gosta Leonard\"\n",
      "\"Sandstrom, Miss. Marguerite Rut\"\n",
      "\"Bonnell, Miss. Elizabeth\"\n",
      "\"Vestrom, Miss. Hulda Amanda Adolfina\"\n",
      "\"Rice, Master. Eugene\"\n",
      "\"McGowan, Miss. Anna \"\"Annie\"\"\"\n",
      "\"Palsson, Miss. Torborg Danira\"\n",
      "\"O'Dwyer, Miss. Ellen \"\"Nellie\"\"\"\n",
      "\"Uruchurtu, Don. Manuel E\"\n",
      "\"Glynn, Miss. Mary Agatha\"\n",
      "\"Vander Planke, Miss. Augusta Maria\"\n",
      "\"Nicola-Yarred, Miss. Jamila\"\n",
      "\"Laroche, Miss. Simonne Marie Anne Andree\"\n",
      "\"Devaney, Miss. Margaret Delia\"\n",
      "\"O'Driscoll, Miss. Bridget\"\n",
      "\"Panula, Master. Juha Niilo\"\n",
      "\"Rugg, Miss. Emily\"\n",
      "\"West, Miss. Constance Mirium\"\n",
      "\"Goodwin, Master. William Frederick\"\n"
     ]
    }
   ],
   "source": [
    "// build incrementally a set with known keys\n",
    "HashSet<String> keys = new HashSet<>();\n",
    "keys.add(\"Mrs\");\n",
    "keys.add(\"Mr\");\n",
    "\n",
    "// filter out names with known keys\n",
    "// print first twenty to inspect and see other keys\n",
    "train.rvar(\"Name\").stream().mapToString().filter(txt -> {\n",
    "    for(String key : keys)\n",
    "        if(txt.contains(\" \" + key + \". \"))\n",
    "            return false;\n",
    "    return true;\n",
    "}).limit(20).forEach(WS::println);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduced our search and found other titles like `\"Miss\"`, `\"Master\"`. We arrive at the following set of keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?  Mr Mrs Miss Master Don Rev Dr Mme Ms Major Lady Sir Mlle Col Capt Countess Jonkheer \n",
      "-  -- --- ---- ------ --- --- -- --- -- ----- ---- --- ---- --- ---- -------- -------- \n",
      "0 517 125 182    40    1   6  7   1  1    2    1    1   2    2   1      1        1     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "HashSet<String> keys = new HashSet<>();\n",
    "keys.addAll(Arrays.asList(\"Mrs\", \"Mme\", \"Lady\", \"Countess\", \"Mr\", \"Sir\",\"Don\", \"Ms\", \"Miss\", \n",
    "\"Mlle\", \"Master\", \"Dr\",\"Col\", \"Major\", \"Jonkheer\", \"Capt\", \"Rev\"));\n",
    "\n",
    "VarNominal title = train.rvar(\"Name\").stream().mapToString().map(txt -> {\n",
    "    for(String key : keys)\n",
    "        if(txt.contains(\" \" + key + \". \"))\n",
    "            return key;\n",
    "    return \"?\";\n",
    "}).collect(VarNominal.collector());\n",
    "DVector.fromCounts(true, title).printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that we exhausted training data. This is enough. It is possible that in test data to appear new titles. We will consider them missing values. That is why we return `\"?\"` when no matching is found. Another thing to notice is that some of the labels have few number of appearances. We will merge them in a greater category.\n",
    "\n",
    "Another useful feature built in `rapaio` is filters. There are two types of filters: variable filters and frame filters. The nice part of frame filters is that learning algorithms are able to use frame filters naturally, in order to make feature transformations on data. This kind of filters are called *input filters* from the learning algorithm perspective. It is important that you know that input filters transforms features before train phase and also on fit phase.\n",
    "\n",
    "We will build a learning filter to create a new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TitleFilter implements FFilter {\n",
    "    private HashMap<String, String[]> replaceMap = new HashMap<>();\n",
    "    private Function<String, String> titleFun = txt -> {\n",
    "        for (Map.Entry<String, String[]> e : replaceMap.entrySet()) {\n",
    "            for (int i = 0; i < e.getValue().length; i++) {\n",
    "                if (txt.contains(\" \" + e.getValue()[i] + \". \"))\n",
    "                    return e.getKey();\n",
    "                }\n",
    "            }\n",
    "        return \"?\";\n",
    "    };\n",
    "\n",
    "    @Override\n",
    "    public void fit(Frame df) {\n",
    "        replaceMap.put(\"Mrs\", new String[]{\"Mrs\", \"Mme\", \"Lady\", \"Countess\"});\n",
    "        replaceMap.put(\"Mr\", new String[]{\"Mr\", \"Sir\", \"Don\", \"Ms\"});\n",
    "        replaceMap.put(\"Miss\", new String[]{\"Miss\", \"Mlle\"});\n",
    "        replaceMap.put(\"Master\", new String[]{\"Master\"});\n",
    "        replaceMap.put(\"Dr\", new String[]{\"Dr\"});\n",
    "        replaceMap.put(\"Military\", new String[]{\"Col\", \"Major\", \"Jonkheer\", \"Capt\"});\n",
    "        replaceMap.put(\"Rev\", new String[]{\"Rev\"});\n",
    "    }\n",
    "\n",
    "    @Override\n",
    "    public Frame apply(Frame df) {\n",
    "        VarNominal title = VarNominal.empty(0, new ArrayList<>(replaceMap.keySet())).withName(\"Title\");\n",
    "        df.rvar(\"Name\").stream().mapToString().forEach(name -> title.addLabel(titleFun.apply(name)));\n",
    "        return df.bindVars(title);\n",
    "    }\n",
    "    \n",
    "    @Override\n",
    "    public TitleFilter newInstance() {\n",
    "        return new TitleFilter();\n",
    "    }\n",
    "    \n",
    "    public String[] varNames() {\n",
    "        return new String[0];\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a new random forest on the reduced data set and also on title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CrossValidation with 10 folds\n",
      "CV  1:  acc=0.822222, mean=0.822222, se=NaN\n",
      "CV  2:  acc=0.741573, mean=0.781898, se=0.057028\n",
      "CV  3:  acc=0.853933, mean=0.805909, se=0.057929\n",
      "CV  4:  acc=0.808989, mean=0.806679, se=0.047324\n",
      "CV  5:  acc=0.752809, mean=0.795905, se=0.047540\n",
      "CV  6:  acc=0.808989, mean=0.798086, se=0.042855\n",
      "CV  7:  acc=0.820225, mean=0.801248, se=0.040006\n",
      "CV  8:  acc=0.842697, mean=0.806429, se=0.039832\n",
      "CV  9:  acc=0.842697, mean=0.810459, se=0.039172\n",
      "CV 10:  acc=0.808989, mean=0.810312, se=0.036934\n",
      "==============\n",
      "Mean accuracy:0.810312\n",
      "SE: 0.036934     (Standard error)\n",
      "CForest model\n",
      "================\n",
      "\n",
      "Description:\n",
      "CForest{runs:100;\n",
      "baggingMode:DISTRIBUTION;\n",
      "oob:false;\n",
      "sampler:Bootstrap(p=1);\n",
      "tree:CTree {varSelector=VarSelector[ALL];\n",
      "minCount=3;\n",
      "maxDepth=-1;\n",
      "tests=INT:NumericBinary,NOMINAL:NominalBinary,BINARY:BinaryBinary,DOUBLE:NumericBinary;\n",
      "func=GiniGain;\n",
      "split=ToAllWeighted;\n",
      "}}\n",
      "\n",
      "Capabilities:\n",
      "types inputs/targets: BINARY,INT,NOMINAL,DOUBLE/NOMINAL\n",
      "counts inputs/targets: [1,1000000] / [1,1]\n",
      "missing inputs/targets: true/false\n",
      "\n",
      "Learned model:\n",
      "input vars: \n",
      "0.      Sex : NOMINAL  | \n",
      "1.   Pclass : NOMINAL  | \n",
      "2. Embarked : NOMINAL  | \n",
      "3.    Title : NOMINAL  | \n",
      "\n",
      "target vars:\n",
      "> Survived : NOMINAL [?,0,1]\n",
      "\n",
      "\n",
      "> Confusion\n",
      "\n",
      "Ac\\Pr |    0    1 | total \n",
      "----- |    -    - | ----- \n",
      "    0 | >468   81 |   549 \n",
      "    1 |   96 >246 |   342 \n",
      "----- |    -    - | ----- \n",
      "total |  564  327 |   891 \n",
      "\n",
      "\n",
      "Complete cases 891 from 891\n",
      "Acc: 0.8013468         (Accuracy )\n",
      "F1:  0.8409704         (F1 score / F-measure)\n",
      "MCC: 0.5768959         (Matthew correlation coefficient)\n",
      "Pre: 0.8297872         (Precision)\n",
      "Rec: 0.852459         (Recall)\n",
      "G:   0.8410467         (G-measure)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RandomSource.setSeed(124);\n",
    "\n",
    "Frame df_train = train.fapply(new TitleFilter());\n",
    "Frame df_test = test.fapply(new TitleFilter());\n",
    "\n",
    "CForest rf = CForest.newRF()\n",
    ".withClassifier(CTree.newCART().withMinCount(3)).withRuns(100);\n",
    "\n",
    "CEvaluation.cv(df_train.mapVars(\"Survived,Sex,Pclass,Embarked,Age,Fare,SibSp,Parch\"), \"Survived\", rf, 10);\n",
    "\n",
    "rf.fit(df_train.mapVars(\"Survived,Sex,Pclass,Embarked,Title\"), \"Survived\");\n",
    "rf.printSummary();\n",
    "\n",
    "ClassResult fit = rf.predict(df_test);\n",
    "\n",
    "new Confusion(train.rvar(\"Survived\"), rf.predict(df_train).firstClasses()).printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that looks slightly better than our best classifier. We submit that to kaggle to see if there is any improvement.\n",
    "\n",
    "![Progress after incorporating title name into input features](images/titanic-rf1-model.png)\n",
    "\n",
    "### 5.2 Other features\n",
    "\n",
    "There are various authors which published their work on solving this kaggle competition. Most interesting part of their work is the feature engineering section. I developed here some ideas in order to show how one can do this with the library.\n",
    "\n",
    "#### 5.2.1 Family size\n",
    "\n",
    "Using directly `\"SibSp\"` and `\"Parch\"` fields yields no value for a random forest classifier. Studying this two features it looks like those values can be both combined into a single one by summation. This would give us a family size estimator.\n",
    "\n",
    "In order to have an idea of the performance of this new estimator I used a ChiSquare independence test. The idea is to study if those features taken separately worth less than combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ChiSqIndependence\n",
      "\n",
      "Pearson's Chi-squared test with Yates' continuity correction\n",
      "\n",
      "X-squared = 31.9279202, df = 6, p-value = 0.000017\n",
      "\n",
      "Observed data:\n",
      "      1.0 0.0 3.0 4.0 2.0 5.0 8.0 total \n",
      "    0  97 398 12  15  15   5   7   549  \n",
      "    1 112 210  4   3  13   0   0   342  \n",
      "total 209 608 16  18  28   5   7   891  \n",
      "\n",
      "Expected data:\n",
      "              1.0         0.0        3.0        4.0        2.0       5.0       8.0 total \n",
      "    0 128.7777778 374.6262626  9.8585859 11.0909091 17.2525253 3.0808081 4.3131313  549  \n",
      "    1  80.2222222 233.3737374  6.1414141  6.9090909 10.7474747 1.9191919 2.6868687  342  \n",
      "total 209         608         16         18         28         5         7          891  \n",
      "\n",
      "\n",
      "> ChiSqIndependence\n",
      "\n",
      "Pearson's Chi-squared test with Yates' continuity correction\n",
      "\n",
      "X-squared = 23.3892671, df = 6, p-value = 0.000676\n",
      "\n",
      "Observed data:\n",
      "      0.0 1.0 2.0 5.0 3.0 4.0 6.0 total \n",
      "    0 445  53 40   4   2   4   1   549  \n",
      "    1 233  65 40   1   3   0   0   342  \n",
      "total 678 118 80   5   5   4   1   891  \n",
      "\n",
      "Expected data:\n",
      "              0.0         1.0        2.0       5.0       3.0       4.0       6.0 total \n",
      "    0 417.7575758  72.7070707 49.2929293 3.0808081 3.0808081 2.4646465 0.6161616  549  \n",
      "    1 260.2424242  45.2929293 30.7070707 1.9191919 1.9191919 1.5353535 0.3838384  342  \n",
      "total 678         118         80         5         5         4         1          891  \n",
      "\n",
      "\n",
      "> ChiSqIndependence\n",
      "\n",
      "Pearson's Chi-squared test with Yates' continuity correction\n",
      "\n",
      "X-squared = 72.6627665, df = 8, p-value =   1.45e-12\n",
      "\n",
      "Observed data:\n",
      "        2   1  5   3  7  6  4 8 11 total \n",
      "    0  72 374 12  43  8 19  8 6 7   549  \n",
      "    1  89 163  3  59  4  3 21 0 0   342  \n",
      "total 161 537 15 102 12 22 29 6 7   891  \n",
      "\n",
      "Expected data:\n",
      "                2 \n",
      "    0  99.2020202 \n",
      "    1  61.7979798 \n",
      "total 161         \n",
      "\n",
      "          1          5 \n",
      "330.8787879  9.2424242 \n",
      "206.1212121  5.7575758 \n",
      "537         15         \n",
      "\n",
      "          3          7 \n",
      " 62.8484848  7.3939394 \n",
      " 39.1515152  4.6060606 \n",
      "102         12         \n",
      "\n",
      "         6          4 \n",
      "13.5555556 17.8686869 \n",
      " 8.4444444 11.1313131 \n",
      "22         29         \n",
      "\n",
      "        8        11 \n",
      "3.6969697 4.3131313 \n",
      "2.3030303 2.6868687 \n",
      "6         7         \n",
      "\n",
      "total \n",
      " 549  \n",
      " 342  \n",
      " 891  \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// convert sibsp and parch to nominal types to be able to use a chi-square test\n",
    "Var sibsp = VarNominal.from(train.rowCount(), row -> train.getLabel(row, \"SibSp\"));\n",
    "Var parch = VarNominal.from(train.rowCount(), row -> train.getLabel(row, \"Parch\"));\n",
    "\n",
    "// test individually each feature\n",
    "ChiSqIndependence.from(train.rvar(\"Survived\"), sibsp, true).printSummary();\n",
    "ChiSqIndependence.from(train.rvar(\"Survived\"), parch, true).printSummary();\n",
    "\n",
    "// build a combined feature by summation, as nominal\n",
    "VarNominal familySize = VarNominal.from(train.rowCount(),\n",
    "row -> \"\" + (1 + train.getInt(row, \"SibSp\") + train.getInt(row, \"Parch\")));\n",
    "\n",
    "// run the chi-square test on sumation\n",
    "ChiSqIndependence.from(train.rvar(\"Survived\"), familySize, true).printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How we can interpret the result? The test says that each feature brings value separately. The *p-value* associated with both test could be considered significant. That means there are string evidence that those features are not independent of target class. As a conclusion, those features are useful. The last test is made for their summation. It looks like he test is more significant than the previous two. As a consequence we can use the summation instead of those two values taken independently.\n",
    "\n",
    "### 5.3 Cabin and Ticket\n",
    "\n",
    "It seems that cabin and ticket denominations are not useful as they are. There are some various reasons why not to do so. First of all they have many missing values. But a stringer reason is that both have too many levels to contain solid generalization base for learning.\n",
    "\n",
    "If we take only the first letter from each of those two fields, more generalization can happen. This is probably due to the fact that perhaps there is some localization information encoded in those. Perhaps information about the deck, the comfort level, auxiliary functions is encoded in there. As a conclusion it worth a try so we proceed with this thing.\n",
    "\n",
    "To combine all those things we can do it in a single filter or into many filters applied on data. I chose to do a single custom filter to solve all those problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFilter implements FFilter {\n",
    "\n",
    "    private HashMap<String, String[]> replaceMap = new HashMap<>();\n",
    "    private Function<String, String> titleFun = txt -> {\n",
    "        for (Map.Entry<String, String[]> e : replaceMap.entrySet()) {\n",
    "            for (int i = 0; i < e.getValue().length; i++) {\n",
    "                if (txt.contains(\" \" + e.getValue()[i] + \". \"))\n",
    "                    return e.getKey();\n",
    "            }\n",
    "        }\n",
    "        return \"?\";\n",
    "    };\n",
    "\n",
    "    public void fit(Frame df) {\n",
    "        replaceMap.put(\"Mrs\", new String[]{\"Mrs\", \"Mme\", \"Lady\", \"Countess\"});\n",
    "        replaceMap.put(\"Mr\", new String[]{\"Mr\", \"Sir\", \"Don\", \"Ms\"});\n",
    "        replaceMap.put(\"Miss\", new String[]{\"Miss\", \"Mlle\"});\n",
    "        replaceMap.put(\"Master\", new String[]{\"Master\"});\n",
    "        replaceMap.put(\"Dr\", new String[]{\"Dr\"});\n",
    "        replaceMap.put(\"Military\", new String[]{\"Col\", \"Major\", \"Jonkheer\", \"Capt\"});\n",
    "        replaceMap.put(\"Rev\", new String[]{\"Rev\"});\n",
    "    }\n",
    "\n",
    "    public Frame apply(Frame df) {\n",
    "        VarNominal title = VarNominal.empty(0, new ArrayList<>(replaceMap.keySet())).withName(\"Title\");\n",
    "        df.rvar(\"Name\").stream().mapToString().forEach(name -> title.addLabel(titleFun.apply(name)));\n",
    "\n",
    "        Var famSize = VarDouble.from(df.rowCount(), row ->\n",
    "        1.0 + df.getInt(row, \"SibSp\") + df.getInt(row, \"Parch\")).withName(\"FamilySize\");\n",
    "\n",
    "        Var ticket = VarNominal.from(df.rowCount(), row ->\n",
    "        df.isMissing(row, \"Ticket\") ? \"?\" : df.getLabel(row, \"Ticket\")\n",
    "        .substring(0, 1).toUpperCase()\n",
    "        ).withName(\"Ticket\");\n",
    "\n",
    "        Var cabin = VarNominal.from(df.rowCount(), row ->\n",
    "        df.isMissing(row, \"Cabin\") ? \"?\" : (df.getLabel(row, \"Cabin\")\n",
    "        .substring(0, 1).toUpperCase())\n",
    "        ).withName(\"Cabin\");\n",
    "\n",
    "        return df.removeVars(\"Ticket,Cabin\").bindVars(famSize, ticket, cabin, title).copy();\n",
    "    }\n",
    "\n",
    "    public CustomFilter newInstance() { return new CustomFilter(); }\n",
    "    \n",
    "    public String[] varNames() { return new String[0]; }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Another try with random forests\n",
    "\n",
    "So we have some new features and we look to learn from them. We can use a previous classifier like random forests to test it before submit.\n",
    "\n",
    "But we know that we are in danger to overfit is rf. One idea is to transform numeric features into nominal ones by a process named discretization. For this purpose we use a filter from the library called `FQuantileDiscrete`. This filter computes a given number of quantile intervals and put labels according with those intervals on numerical values. Let's see how we proceed and how the data looks like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Summary\n",
      "=============\n",
      "* rowCount: 891\n",
      "* complete: 183/891\n",
      "* varCount: 12\n",
      "* varNames: \n",
      "\n",
      " 0.   Survived : nominal |  4.      SibSp : nominal |  8. FamilySize : nominal | \n",
      " 1.     Pclass : nominal |  5.      Parch : nominal |  9.     Ticket : nominal | \n",
      " 2.        Sex : nominal |  6.       Fare : nominal | 10.      Cabin : nominal | \n",
      " 3.        Age : nominal |  7.   Embarked : nominal | 11.      Title : nominal | \n",
      "\n",
      "Survived   Pclass           Sex            Age         SibSp         Parch                 Fare    Embarked \n",
      " 0 : 549  3 : 491    male : 577  31.8~36 :  91  -Inf~0 : 608  -Inf~0 : 678     7.854~8.05 : 106     S : 644 \n",
      " 1 : 342  1 : 216  female : 314    14~19 :  87   0~Inf : 283   0~Inf : 213      -Inf~7.55 :  92     C : 168 \n",
      "          2 : 184                  41~50 :  78                                  27~39.688 :  91     Q :  77 \n",
      "                                 -Inf~14 :  77                                  21.679~27 :  89  NA's :   2 \n",
      "                                   22~25 :  70                              39.688~77.958 :  89             \n",
      "                                 (Other) : 244                              14.454~21.679 :  88             \n",
      "                                    NA's : 177                                    (Other) : 336             \n",
      "  FamilySize         Ticket          Cabin          Title \n",
      "-Inf~1 : 537        3 : 301        C :  59       Mr : 520 \n",
      "   1~2 : 161        2 : 183        B :  47     Miss : 184 \n",
      "   2~3 : 102        1 : 146        D :  33      Mrs : 128 \n",
      " 3~Inf :  91        P :  65        E :  32   Master :  40 \n",
      "                    S :  65        A :  15       Dr :   7 \n",
      "                    C :  47  (Other) :   5      Rev :   6 \n",
      "              (Other) :  84     NA's : 687  (Other) :   6 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "FFilter[] inputFilters = new FFilter[]{\n",
    "new CustomFilter(),\n",
    "FQuantileDiscrete.split(VRange.of(\"Age\"), 10),\n",
    "FQuantileDiscrete.split(VRange.of(\"Fare\"), 10),\n",
    "FQuantileDiscrete.split(VRange.of(\"SibSp\"), 3),\n",
    "FQuantileDiscrete.split(VRange.of(\"Parch\"), 3),\n",
    "FQuantileDiscrete.split(VRange.of(\"FamilySize\"), 8),\n",
    "FRemoveVars.remove(VRange.of(\"PassengerId,Name\"))\n",
    "};\n",
    "\n",
    "// print a summary of the transformed data\n",
    "train.fapply(inputFilters).printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `\"Age\"` values are now intervals and still $177$ missing values.\n",
    "\n",
    "The values chosen for quantile numbers is more or less arbitrary. There is no *good* numbers in general, only for some specific purposes.\n",
    "\n",
    "As promised, we will give a try to another random forest to see if it can better generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CrossValidation with 10 folds\n",
      "CV  1:  acc=0.844444, mean=0.844444, se=NaN\n",
      "CV  2:  acc=0.820225, mean=0.832335, se=0.017126\n",
      "CV  3:  acc=0.797753, mean=0.820807, se=0.023351\n",
      "CV  4:  acc=0.853933, mean=0.829089, se=0.025256\n",
      "CV  5:  acc=0.831461, mean=0.829563, se=0.021898\n",
      "CV  6:  acc=0.820225, mean=0.828007, se=0.019953\n",
      "CV  7:  acc=0.764045, mean=0.818869, se=0.030269\n",
      "CV  8:  acc=0.808989, mean=0.817634, se=0.028241\n",
      "CV  9:  acc=0.831461, mean=0.819170, se=0.026816\n",
      "CV 10:  acc=0.842697, mean=0.821523, se=0.026354\n",
      "==============\n",
      "Mean accuracy:0.821523\n",
      "SE: 0.026354     (Standard error)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8215230961298376"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomSource.setSeed(123);\n",
    "\n",
    "CForest rf3 = CForest.newRF().withRuns(200);\n",
    "\n",
    "train = train.fapply(inputFilters);\n",
    "\n",
    "rf3.fit(train, \"Survived\");\n",
    "CEvaluation.cv(train, \"Survived\", rf3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried some ideas to make the forest to generalize better.\n",
    "\n",
    "* Smaller bootstrap percentage - this could lead to increased independence of trees\n",
    "* Use `GainRatio` as purity function because sometimes is more conservative\n",
    "* Use `MinGain` to avoid growing trees to have many leaves with a single instance\n",
    "* Use `mCols=4`, number of variables used for testing - more than default value, to improve the quality of each tree\n",
    "\n",
    "These are the results. At a first look might seem like an astonishing result. But we know that the irreducible error for this data set is high and is close to $$0.2$$. It seems obvious that we failed to reduce the variance and we still overfit a lot using this construct. Since this is a tutorial I will not insist on improving this model, but I think that even if it would be improved, the gain would be very small. Perhaps another approach would be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SVM model\n",
    "\n",
    "SVM (Support Vector Machines) is a nice framework to test new ideas for various types of problems. The power of SMVs comes from their kernels. A kernel is basically a transformation of the original space generated by the input features into another space, often with more dimensions. It's like a feature engineering in a singe function.\n",
    "\n",
    "But SVMs have a practical problem. The features needs to be numeric and does not allows missing values. This is not a constrain on the algorithm itself. At any moment one can build a kernel for nominal features. But the implemented ones allows only numeric non missing values and is much simpler to shape our data into this format.\n",
    "\n",
    "How can we do that?\n",
    "\n",
    "### 6.1 Data preparation\n",
    "\n",
    "We can use a filter to impute data for missing values. The filter we use is an imputation with a classifier of imputation with a regression. The logic is the following: train a classifier from a specified set of input features to predict the field with missing values. The data set inside the filter is filtered to contain only instances with non-missing target values.\n",
    "\n",
    "After we impute the missing values we encode nominal features into numeric features. We can accomplish this task using, again, another filter for this purpose. The name of this filter is `FOneHotEncoding`. What it does is to create a number of numeric feature for level of the nominal variable. Than the values on these numeric variables receives the value of the indicator function. We have $1$ if the level equals the numeric variable's name, $0$ otherwise.\n",
    "\n",
    "After we have numerical variables, is better to make all the variables to be in the same range. This is not a requirement for SMVs in general. The meaning is to give same weight to all the involved variables. As a side effect it makes the algorithm to run faster. This is due to the fact that the convex optimization problem has smaller chances to have a close-to-flat big surfaces.\n",
    "\n",
    "Finally, we will remove the not used variables from the frame in order to be prepared for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Adapt to current version of the library the rest of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```java\n",
    "FFilter[] inputFilters = new FFilter[]{\n",
    "new CustomFilter(),\n",
    "new FFImputeWithRegression(RForest.newRF().withRuns(100),\n",
    "new VarRange(\"Age,Pclass,Embarked,Sex,Fare,Title\"), \"Age\"),\n",
    "new FFImputeByClassifier(CForest.newRF().withRuns(10),\n",
    "new VarRange(\"Embarked,Age,Pclass,Sex,Title\"), \"Embarked\"),\n",
    "new FFImputeByClassifier(CForest.newRF().withRuns(100),\n",
    "new VarRange(\"Age,Pclass,Embarked,Sex,Fare,Ticket\"), \"Ticket\"),\n",
    "new FFImputeByClassifier(CForest.newRF().withRuns(100),\n",
    "new VarRange(\"Age,Pclass,Embarked,Sex,Fare,Cabin\"), \"Cabin\"),\n",
    "\n",
    "new FFOneHotEncoding(\"Sex,Embarked,Title,Cabin\"),\n",
    "new FFOneHotEncoding(\"Ticket\"),\n",
    "new FFStandardize(\"all\"),\n",
    "\n",
    "new FFRemoveVars(\"PassengerId,Name,SibSp,Parch\")\n",
    "};\n",
    "train.applyFilters(inputFilters).printSummary()\n",
    "```\n",
    "\n",
    "```\n",
    "Frame Summary\n",
    "=============\n",
    "* rowCount: 891\n",
    "* complete: 891/891\n",
    "* varCount: 41\n",
    "* varNames:\n",
    "\n",
    "0. Survived : nom | 14. Ticket.3 : num | 28. Cabin.G : num |\n",
    "1. Pclass : num | 15. Ticket.2 : num | 29. Cabin.D : num |\n",
    "2. Sex.male : num | 16. Ticket.C : num | 30. Cabin.A : num |\n",
    "3. Sex.female : num | 17. Ticket.7 : num | 31. Cabin.B : num |\n",
    "4. Age : num | 18. Ticket.W : num | 32. Cabin.F : num |\n",
    "5. Fare : num | 19. Ticket.4 : num | 33. Cabin.T : num |\n",
    "6. Embarked.S : num | 20. Ticket.F : num | 34. Title.Master : num |\n",
    "7. Embarked.C : num | 21. Ticket.L : num | 35. Title.Rev : num |\n",
    "8. Embarked.Q : num | 22. Ticket.9 : num | 36. Title.Mr : num |\n",
    "9. FamilySize : num | 23. Ticket.6 : num | 37. Title.Miss : num |\n",
    "10. Ticket.A : num | 24. Ticket.5 : num | 38. Title.Dr : num |\n",
    "11. Ticket.P : num | 25. Ticket.8 : num | 39. Title.Mrs : num |\n",
    "12. Ticket.S : num | 26. Cabin.C : num | 40. Title.Military : num |\n",
    "13. Ticket.1 : num | 27. Cabin.E : num |\n",
    "\n",
    "Survived Pclass Sex.male Sex.female Age\n",
    "0 : 549 Min. : -1.565 Min. : -1.355 Min. : -0.737 Min. : -2.781\n",
    "1 : 342 1st Qu. : -0.369 1st Qu. : -1.355 1st Qu. : -0.737 1st Qu. : -0.336\n",
    "Median : 0.827 Median : 0.737 Median : -0.737 Median : -0.113\n",
    "Mean : -0.000 Mean : -0.000 Mean : 0.000 Mean : 0.000\n",
    "2nd Qu. : 0.827 2nd Qu. : 0.737 2nd Qu. : 1.355 2nd Qu. : 0.483\n",
    "Max. : 0.827 Max. : 0.737 Max. : 1.355 Max. : 2.929\n",
    "Fare Embarked.S Embarked.C Embarked.Q FamilySize\n",
    "Min. : -0.648 Min. : -1.632 Min. : -0.482 Min. : -0.303 Min. : -0.561\n",
    "1st Qu. : -0.489 1st Qu. : -1.632 1st Qu. : -0.482 1st Qu. : -0.303 1st Qu. : -0.561\n",
    "Median : -0.357 Median : 0.612 Median : -0.482 Median : -0.303 Median : -0.561\n",
    "Mean : 0.000 Mean : 0.000 Mean : 0.000 Mean : 0.000 Mean : -0.000\n",
    "2nd Qu. : -0.024 2nd Qu. : 0.612 2nd Qu. : -0.482 2nd Qu. : -0.303 2nd Qu. : 0.059\n",
    "Max. : 9.662 Max. : 0.612 Max. : 2.073 Max. : 3.297 Max. : 5.637\n",
    "Ticket.A Ticket.P Ticket.S Ticket.1 Ticket.3\n",
    "Min. : -0.139 Min. : -0.280 Min. : -0.251 Min. : -0.433 Min. : -0.767\n",
    "1st Qu. : -0.139 1st Qu. : -0.280 1st Qu. : -0.251 1st Qu. : -0.433 1st Qu. : -0.767\n",
    "Median : -0.139 Median : -0.280 Median : -0.251 Median : -0.433 Median : -0.767\n",
    "Mean : -0.000 Mean : -0.000 Mean : -0.000 Mean : -0.000 Mean : 0.000\n",
    "2nd Qu. : -0.139 2nd Qu. : -0.280 2nd Qu. : -0.251 2nd Qu. : -0.433 2nd Qu. : 1.303\n",
    "Max. : 7.166 Max. : 3.563 Max. : 3.974 Max. : 2.305 Max. : 1.303\n",
    "Ticket.2 Ticket.C Ticket.7 Ticket.W Ticket.4\n",
    "Min. : -0.508 Min. : -0.261 Min. : -0.101 Min. : -0.101 Min. : -0.106\n",
    "1st Qu. : -0.508 1st Qu. : -0.261 1st Qu. : -0.101 1st Qu. : -0.101 1st Qu. : -0.106\n",
    "Median : -0.508 Median : -0.261 Median : -0.101 Median : -0.101 Median : -0.106\n",
    "Mean : 0.000 Mean : -0.000 Mean : -0.000 Mean : 0.000 Mean : 0.000\n",
    "2nd Qu. : -0.508 2nd Qu. : -0.261 2nd Qu. : -0.101 2nd Qu. : -0.101 2nd Qu. : -0.106\n",
    "Max. : 1.966 Max. : 3.823 Max. : 9.894 Max. : 9.894 Max. : 9.381\n",
    "Ticket.F Ticket.L Ticket.9 Ticket.6 Ticket.5\n",
    "Min. : -0.082 Min. : -0.067 Min. : 0.000 Min. : -0.067 Min. : -0.034\n",
    "1st Qu. : -0.082 1st Qu. : -0.067 1st Qu. : 0.000 1st Qu. : -0.067 1st Qu. : -0.034\n",
    "Median : -0.082 Median : -0.067 Median : 0.000 Median : -0.067 Median : -0.034\n",
    "Mean : -0.000 Mean : -0.000 Mean : 0.000 Mean : -0.000 Mean : -0.000\n",
    "2nd Qu. : -0.082 2nd Qu. : -0.067 2nd Qu. : 0.000 2nd Qu. : -0.067 2nd Qu. : -0.034\n",
    "Max. : 12.138 Max. : 14.883 Max. : 0.000 Max. : 14.883 Max. : 29.816\n",
    "Ticket.8 Cabin.C Cabin.E Cabin.G Cabin.D\n",
    "Min. : -0.047 Min. : -0.379 Min. : -0.615 Min. : -0.301 Min. : -0.499\n",
    "1st Qu. : -0.047 1st Qu. : -0.379 1st Qu. : -0.615 1st Qu. : -0.301 1st Qu. : -0.499\n",
    "Median : -0.047 Median : -0.379 Median : -0.615 Median : -0.301 Median : -0.499\n",
    "Mean : -0.000 Mean : -0.000 Mean : 0.000 Mean : -0.000 Mean : 0.000\n",
    "2nd Qu. : -0.047 2nd Qu. : -0.379 2nd Qu. : 1.623 2nd Qu. : -0.301 2nd Qu. : -0.499\n",
    "Max. : 21.071 Max. : 2.636 Max. : 1.623 Max. : 3.321 Max. : 2.000\n",
    "Cabin.A Cabin.B Cabin.F Cabin.T Title.Master\n",
    "Min. : -0.159 Min. : -0.269 Min. : -0.538 Min. : 0.000 Min. : -0.217\n",
    "1st Qu. : -0.159 1st Qu. : -0.269 1st Qu. : -0.538 1st Qu. : 0.000 1st Qu. : -0.217\n",
    "Median : -0.159 Median : -0.269 Median : -0.538 Median : 0.000 Median : -0.217\n",
    "Mean : 0.000 Mean : -0.000 Mean : 0.000 Mean : 0.000 Mean : 0.000\n",
    "2nd Qu. : -0.159 2nd Qu. : -0.269 2nd Qu. : -0.538 2nd Qu. : 0.000 2nd Qu. : -0.217\n",
    "Max. : 6.281 Max. : 3.719 Max. : 1.858 Max. : 0.000 Max. : 4.610\n",
    "Title.Rev Title.Mr Title.Miss Title.Dr Title.Mrs\n",
    "Min. : -0.082 Min. : -1.183 Min. : -0.510 Min. : -0.089 Min. : -0.409\n",
    "1st Qu. : -0.082 1st Qu. : -1.183 1st Qu. : -0.510 1st Qu. : -0.089 1st Qu. : -0.409\n",
    "Median : -0.082 Median : 0.844 Median : -0.510 Median : -0.089 Median : -0.409\n",
    "Mean : -0.000 Mean : -0.000 Mean : -0.000 Mean : 0.000 Mean : -0.000\n",
    "2nd Qu. : -0.082 2nd Qu. : 0.844 2nd Qu. : -0.510 2nd Qu. : -0.089 2nd Qu. : -0.409\n",
    "Max. : 12.138 Max. : 0.844 Max. : 1.959 Max. : 11.231 Max. : 2.440\n",
    "Title.Military\n",
    "Min. : -0.082\n",
    "1st Qu. : -0.082\n",
    "Median : -0.082\n",
    "Mean : 0.000\n",
    "2nd Qu. : -0.082\n",
    "Max. : 12.138\n",
    "```\n",
    "\n",
    "There is a lot of content. Notice that we have numerical variables for each ticket first letter, title, cabin first letter, etc.\n",
    "\n",
    "## Train a polynomial SVM\n",
    "\n",
    "A linear kernel is a polynomial kernel with degree 1. We let the $$C$$ parameter to the default value which is $$1$$.\n",
    "\n",
    "```java\n",
    "Classifier model = new BinarySMO()\n",
    ".withInputFilters(inputFilters)\n",
    ".withC(0.0001)\n",
    ".withKernel(new PolyKernel(1));\n",
    "model.train(train, \"Survived\");\n",
    "CFit fit = model.fit(test);\n",
    "new Confusion(train.getVar(\"Survived\"), model.fit(train).firstClasses()).printSummary();\n",
    "Csv.instance().withQuotes(false).write(SolidFrame.wrapOf(\n",
    "test.getVar(\"PassengerId\"),\n",
    "fit.firstClasses().withName(\"Survived\")\n",
    "), root + \"svm1-submit.csv\");\n",
    "cv(train, model);\n",
    "```\n",
    "```\n",
    "> Confusion\n",
    "\n",
    "Ac\\Pr | 0 1 | total\n",
    "----- | - - | -----\n",
    "0 | >292 257 | 549\n",
    "1 | 41 >301 | 342\n",
    "----- | - - | -----\n",
    "total | 333 558 | 891\n",
    "\n",
    "\n",
    "Complete cases 891 from 891\n",
    "Acc: 0.6655443 (Accuracy )\n",
    "F1: 0.6621315 (F1 score / F-measure)\n",
    "MCC: 0.4141426 (Matthew correlation coefficient)\n",
    "Pre: 0.8768769 (Precision)\n",
    "Rec: 0.5318761 (Recall)\n",
    "G: 0.6829274 (G-measure)\n",
    "Cross validation 10-fold\n",
    "CV fold: 1, acc: 0.666667, mean: 0.666667, se: NaN\n",
    "CV fold: 2, acc: 0.764045, mean: 0.715356, se: 0.068857\n",
    "CV fold: 3, acc: 0.775281, mean: 0.735331, se: 0.059730\n",
    "CV fold: 4, acc: 0.707865, mean: 0.728464, se: 0.050666\n",
    "CV fold: 5, acc: 0.719101, mean: 0.726592, se: 0.044077\n",
    "CV fold: 6, acc: 0.696629, mean: 0.721598, se: 0.041278\n",
    "CV fold: 7, acc: 0.842697, mean: 0.738898, se: 0.059286\n",
    "CV fold: 8, acc: 0.730337, mean: 0.737828, se: 0.054972\n",
    "CV fold: 9, acc: 0.752809, mean: 0.739492, se: 0.051663\n",
    "CV fold:10, acc: 0.808989, mean: 0.746442, se: 0.053437\n",
    "=================\n",
    "mean: 0.746442, se: 0.053437\n",
    "```\n",
    "\n",
    "The results are not promising. This is better than random but it is not enough for our purpose. There are some explanations for this result. First one could be that if the space would be linear, than the original feature space would be the same as transformed. This means that a classifier as random forest would work well if the linear svm would have worked. This might not be true in general, but in this case it looks like a good explanation. We need to be more flexible.\n",
    "\n",
    "To increase the flexibility of the model and to allow features to interact with one another we change the degree of the polynomial kernel. This time we will use `degree=3`. Also, we use $$C=0.0001$$ to allow for some errors. This parameter is the factor of the slack regularization constraints of the SVM optimization problem. The bigger the value the more is the penalty for wrong decisions. If the space would be linear separable than one can theoretically set this value as high as possible. But we know it is not. Also we know that we have plenty of irreducible error. As a consequence, it looks like we should decrease the value of this parameter.\n",
    "\n",
    "```java\n",
    "Classifier model = new BinarySMO()\n",
    ".withInputFilters(inputFilters)\n",
    ".withC(0.0001)\n",
    ".withKernel(new PolyKernel(1));\n",
    "model.train(train, \"Survived\");\n",
    "CFit fit = model.fit(test);\n",
    "new Confusion(train.getVar(\"Survived\"), model.fit(train).firstClasses()).printSummary();\n",
    "Csv.instance().withQuotes(false).write(SolidFrame.wrapOf(\n",
    "test.getVar(\"PassengerId\"),\n",
    "fit.firstClasses().withName(\"Survived\")\n",
    "), root + \"svm1-submit.csv\");\n",
    "cv(train, model);\n",
    "```\n",
    "```\n",
    "> Confusion\n",
    "\n",
    "Ac\\Pr | 0 1 | total\n",
    "----- | - - | -----\n",
    "0 | >472 77 | 549\n",
    "1 | 48 >294 | 342\n",
    "----- | - - | -----\n",
    "total | 520 371 | 891\n",
    "\n",
    "\n",
    "Complete cases 891 from 891\n",
    "Acc: 0.8597082 (Accuracy )\n",
    "F1: 0.8830683 (F1 score / F-measure)\n",
    "MCC: 0.7097044 (Matthew correlation coefficient)\n",
    "Pre: 0.9076923 (Precision)\n",
    "Rec: 0.859745 (Recall)\n",
    "G: 0.8833934 (G-measure)\n",
    "Cross validation 10-fold\n",
    "CV fold: 1, acc: 0.822222, mean: 0.822222, se: NaN\n",
    "CV fold: 2, acc: 0.786517, mean: 0.804370, se: 0.025248\n",
    "CV fold: 3, acc: 0.853933, mean: 0.820891, se: 0.033728\n",
    "CV fold: 4, acc: 0.808989, mean: 0.817915, se: 0.028174\n",
    "CV fold: 5, acc: 0.797753, mean: 0.813883, se: 0.026012\n",
    "CV fold: 6, acc: 0.786517, mean: 0.809322, se: 0.025809\n",
    "CV fold: 7, acc: 0.831461, mean: 0.812484, se: 0.025002\n",
    "CV fold: 8, acc: 0.876404, mean: 0.820474, se: 0.032350\n",
    "CV fold: 9, acc: 0.764045, mean: 0.814204, se: 0.035631\n",
    "CV fold:10, acc: 0.820225, mean: 0.814806, se: 0.033647\n",
    "=================\n",
    "mean: 0.814806, se: 0.033647\n",
    "```\n",
    "\n",
    "This time the results are promising. We achieved a training error which is not close to zero and the cross validation errors are close to our desired results. We definitely should try this classifier.\n",
    "\n",
    "![SVM1](images/titanic-svm1-submit.png)\n",
    "\n",
    "We have a better score also on public leader board. Which is very fine. Usually in this competition a score in $$0.75-0.78$$ is fine and one in $$0.78-0.81$$ is excellent.\n",
    "\n",
    "## Tuning manually the SVM\n",
    "\n",
    "We can work more on SVMs. One thing which deserves a try is the radial basis kernel. This is similar with working in an infinite dimensional space! We tried some `RBFKernel` approaches, but much better results gave the `CauchyKernel`. The `CauchyKernel` works in a similar way like a RBF kernel. The difference which sometimes is important is that it is a distribution with tails fatter than Gaussian distribution. This produces an effect of long distance influence. This is reasonable to use in this problem because we know we have noise. We can think that a kernel which acts on wider ranges is better if it is combined with a small value for $$C$$.\n",
    "\n",
    "After some manual tuning we arrived at the following classifier.\n",
    "\n",
    "```java\n",
    "Classifier model = new BinarySMO()\n",
    ".withInputFilters(inputFilters)\n",
    ".withC(1)\n",
    ".withTol(1e-10)\n",
    ".withKernel(new CauchyKernel(25));\n",
    "```\n",
    "```\n",
    "> Confusion\n",
    "\n",
    "Ac\\Pr | 0 1 | total\n",
    "----- | - - | -----\n",
    "0 | >520 29 | 549\n",
    "1 | 104 >238 | 342\n",
    "----- | - - | -----\n",
    "total | 624 267 | 891\n",
    "\n",
    "\n",
    "Complete cases 891 from 891\n",
    "Acc: 0.8507295 (Accuracy )\n",
    "F1: 0.8866155 (F1 score / F-measure)\n",
    "MCC: 0.6826819 (Matthew correlation coefficient)\n",
    "Pre: 0.8333333 (Precision)\n",
    "Rec: 0.9471767 (Recall)\n",
    "G: 0.8884334 (G-measure)\n",
    "Cross validation 10-fold\n",
    "CV fold: 1, acc: 0.811111, mean: 0.811111, se: NaN\n",
    "CV fold: 2, acc: 0.786517, mean: 0.798814, se: 0.017391\n",
    "CV fold: 3, acc: 0.865169, mean: 0.820932, se: 0.040235\n",
    "CV fold: 4, acc: 0.797753, mean: 0.815137, se: 0.034836\n",
    "CV fold: 5, acc: 0.831461, mean: 0.818402, se: 0.031040\n",
    "CV fold: 6, acc: 0.842697, mean: 0.822451, se: 0.029481\n",
    "CV fold: 7, acc: 0.820225, mean: 0.822133, se: 0.026926\n",
    "CV fold: 8, acc: 0.820225, mean: 0.821895, se: 0.024937\n",
    "CV fold: 9, acc: 0.797753, mean: 0.819212, se: 0.024676\n",
    "CV fold:10, acc: 0.831461, mean: 0.820437, se: 0.023585\n",
    "=================\n",
    "mean: 0.820437, se: 0.023585\n",
    "```\n",
    "\n",
    "This classifier has similar results, but there are reasons to believe that it is slightly better than previous. The training error smaller. But we know that training error is not a good estimator. The 10 fold cv is greater. This is a good sign. A better interpretation would be that the gap between those two has shrunken and this is a good thing. A new submit on kaggle follows.\n",
    "\n",
    "![SVM2](images/titanic-svm2-submit.png)\n",
    "\n",
    "Well we are really, really close to our psychological milestone of $$0.8$$. Perhaps some tuning will give more results. This is true in general. However, next section provides you with a better approach which usually provides some gain in accuracy: stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stacking classifiers\n",
    "\n",
    "Using random forests or SVMs did not provided us with a result over $$0.8$$. We have two very different types of models which performed well. For the sole purpose of prediction we use a nice ensemble technique which often provides good prediction performance gain. This technique is called stacking.\n",
    "\n",
    "The idea behind stacking is that one can explore the space of the solutions with different approaches. Each approach (or statistical model) is basically an interpretation of the solution. But often times a proper interpretation is really hard to find. Each interpretation of the solution can have good points and weak points. The idea is to blend those interpretations into a single one in a way that we try somehow to keep what is string from each individual classifier.\n",
    "\n",
    "A stacking classifier take some base learners and train them on training data. The results of the base learners are used as input for a stacking learner. This stacking learner is trained on the output of base learners and target variable and is finally used for prediction.\n",
    "\n",
    "```java\n",
    "Classifier model = new CStacking()\n",
    ".withLearners(\n",
    "new BinarySMO()\n",
    ".withInputFilters(inputFilters)\n",
    ".withC(1)\n",
    ".withTol(1e-10)\n",
    ".withKernel(new CauchyKernel(20)),\n",
    "CForest.newRF()\n",
    ".withInputFilters(inputFilters)\n",
    ".withMCols(4)\n",
    ".withBootstrap(0.07)\n",
    ".withClassifier(CTree.newCART()\n",
    ".withFunction(CTreePurityFunction.GainRatio)\n",
    ".withMinGain(0.001))\n",
    ".withRuns(200)\n",
    ").withStacker(CForest.newRF().withBootstrap(0.3)\n",
    ".withRuns(200)\n",
    ".withClassifier(CTree.newCART()\n",
    ".withFunction(CTreePurityFunction.GainRatio)\n",
    ".withMinGain(0.05)));\n",
    "```\n",
    "\n",
    "Usually one uses a binary logistic regression model but it provided weak results. What looked much better is another random forrest classifier. However the stacking model uses a big value for minimum gain parameter because we want to act as an draft average over the results.\n",
    "\n",
    "```\n",
    "> Confusion\n",
    "\n",
    "Ac\\Pr | 0 1 | total\n",
    "----- | - - | -----\n",
    "0 | >522 27 | 549\n",
    "1 | 87 >255 | 342\n",
    "----- | - - | -----\n",
    "total | 609 282 | 891\n",
    "\n",
    "\n",
    "Complete cases 891 from 891\n",
    "Acc: 0.8720539 (Accuracy )\n",
    "F1: 0.9015544 (F1 score / F-measure)\n",
    "MCC: 0.7281918 (Matthew correlation coefficient)\n",
    "Pre: 0.8571429 (Precision)\n",
    "Rec: 0.9508197 (Recall)\n",
    "G: 0.902767 (G-measure)\n",
    "Cross validation 10-fold\n",
    "CV fold: 1, acc: 0.888889, mean: 0.888889, se: NaN\n",
    "CV fold: 2, acc: 0.786517, mean: 0.837703, se: 0.072388\n",
    "CV fold: 3, acc: 0.842697, mean: 0.839367, se: 0.051267\n",
    "CV fold: 4, acc: 0.820225, mean: 0.834582, se: 0.042940\n",
    "CV fold: 5, acc: 0.808989, mean: 0.829463, se: 0.038908\n",
    "CV fold: 6, acc: 0.853933, mean: 0.833541, se: 0.036206\n",
    "CV fold: 7, acc: 0.786517, mean: 0.826824, se: 0.037527\n",
    "CV fold: 8, acc: 0.898876, mean: 0.835830, se: 0.043082\n",
    "CV fold: 9, acc: 0.831461, mean: 0.835345, se: 0.040326\n",
    "CV fold:10, acc: 0.775281, mean: 0.829338, se: 0.042500\n",
    "=================\n",
    "mean: 0.829338, se: 0.042500\n",
    "```\n",
    "Again, the results are promising. The space between training error and cross validation error is smaller and our expectations grows.\n",
    "\n",
    "![Stacking with a random forest](images/titanic-stacking1-submit.png)\n",
    "\n",
    "Finally our target performance was achieved!!\n",
    "\n",
    "**Note:**\n",
    "\n",
    "The general advice in real life is to not fight for each piece of performance measure. It really depends on the question one wants to answer. Often measures like ROC or partial ROC are much better than error frequency. We fixed this milestone because we know it is possible and because it looks like a psychological difficulty. (that $$0.799$$ is outrageous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "11.0.2+9-LTS"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
